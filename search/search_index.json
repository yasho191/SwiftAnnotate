{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SwiftAnnotate \ud83d\ude80","text":"<p>SwiftAnnotate is a comprehensive auto-labeling tool designed for Text, Image, and Video data. It leverages state-of-the-art (SOTA) Vision Language Models (VLMs) and Large Language Models (LLMs) through a robust annotator-validator pipeline, ensuring high-quality, grounded annotations while minimizing hallucinations. SwiftAnnotate also supports annotations tasks like Object Detection and Segmentation through SOTA CV models like <code>SAM2</code>, <code>YOLOWorld</code>, and <code>OWL-ViT</code>.</p>"},{"location":"#key-features","title":"Key Features \ud83c\udfaf","text":"<ol> <li> <p>Text Processing \ud83d\udcdd Perform classification, summarization, and text generation with state-of-the-art NLP models. Solve real-world problems like spam detection, sentiment analysis, and content creation.</p> </li> <li> <p>Image Analysis \ud83d\uddbc\ufe0f Generate captions for images to provide meaningful descriptions. Classify images into predefined categories with high precision. Detect objects in images using models like YOLOWorld. Achieve pixel-perfect segmentation with SAM2 and OWL-ViT.  </p> </li> <li> <p>Video Processing \ud83c\udfa5 Generate captions for videos with frame-level analysis and temporal understanding Understand video content by detecting scenes and actions effortlessly.  </p> </li> <li> <p>Quality Assurance \u2705 Use a two-stage pipeline for annotation and validation to ensure high data quality. Validate outputs rigorously to maintain reliability before deployment.  </p> </li> <li> <p>Multi-modal Support \ud83c\udf10 Seamlessly process text, images, and videos within a unified framework. Combine data types for powerful multi-modal insights and applications.  </p> </li> <li> <p>Customization \ud83d\udee0\ufe0f Easily extend and adapt the framework to suit specific project needs. Integrate new models and tasks seamlessly with modular architecture.</p> </li> <li> <p>Developer-Friendly \ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc68\u200d\ud83d\udcbb Easy-to-use package and detailed documentation to get started quickly.</p> </li> </ol>"},{"location":"#installation-guide","title":"Installation Guide","text":"<p>To install SwiftAnnotate from PyPI and set up the project environment, follow these steps:  </p> <ol> <li> <p>Install from PyPI </p> <p>Run the following command to install the package directly:  </p> <pre><code>pip install swiftannotate\n</code></pre> </li> <li> <p>For Development (Using Poetry) </p> <p>If you want to contribute or explore the project codebase ensure you have Poetry installed.  Follow the steps given below:</p> <pre><code>git clone https://github.com/yasho191/SwiftAnnotate\ncd SwiftAnnotate\npoetry install\n</code></pre> <p>You're now ready to explore and develop SwiftAnnotate!  </p> </li> </ol>"},{"location":"#annotator-validator-pipeline-for-llms-and-vlms","title":"Annotator-Validator Pipeline for LLMs and VLMs","text":"<p>The annotator-validator pipeline ensures high-quality annotations through a two-stage process:</p> <p>Stage 1: Annotation</p> <ul> <li>Primary LLM/VLM generates initial annotations</li> <li>Configurable model selection (OpenAI, Google Gemini, Anthropic, Mistral, Qwen-VL)</li> </ul> <p>Stage 2: Validation</p> <ul> <li>Secondary model validates initial annotations</li> <li>Cross-checks for hallucinations and factual accuracy</li> <li>Provides confidence scores and correction suggestions</li> <li>Option to regenerate annotations if validation fails</li> <li>Structured output format for consistency</li> </ul> <p>Benefits</p> <ul> <li>Reduced hallucinations through 2 stage verification</li> <li>Higher annotation quality and consistency</li> <li>Automated quality control</li> <li>Traceable annotation process</li> </ul> <p>The pipeline can be customized with different model combinations and validation thresholds based on specific use cases.</p>"},{"location":"#supported-modalities-and-tasks","title":"Supported Modalities and Tasks","text":""},{"location":"#text","title":"Text","text":""},{"location":"#images","title":"Images","text":""},{"location":"#captioning","title":"Captioning","text":"<p>Currently, we support OpenAI, Google-Gemini, Ollama, and Qwen2-VL for image captioning. As Qwen2-VL is not yet available on Ollama it is supported through HuggingFace. To get started quickly refer the code snippets shown below.</p> <p>OpenAI</p> <pre><code>import os\nfrom swiftannotate.image import OpenAIForImageCaptioning\n\ncaption_model = \"gpt-4o\"\nvalidation_model = \"gpt-4o-mini\"\napi_key = \"&lt;YOUR_OPENAI_API_KEY&gt;\"\nBASE_DIR = \"&lt;IMAGE_DIR&gt;\"\nimage_paths = [os.path.join(BASE_DIR, image) for image in os.listdir(BASE_DIR)]\n\nimage_captioning_pipeline = OpenAIForImageCaptioning(\n    caption_model=caption_model,\n    validation_model=validation_model,\n    api_key=api_key,\n    output_file=\"image_captioning_output.json\"\n)\n\nresults = image_captioning_pipeline.generate(image_paths=image_paths)\n</code></pre> <p>Qwen2-VL</p> <p>You can use any version for the Qwen2-VL (7B, 72B) depending on the available resources. vLLM inference is not currently supported but it will be available soon.</p> <pre><code>import os\nfrom transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers import BitsAndBytesConfig\nfrom swiftannotate.image import Qwen2VLForImageCaptioning\n\n# Load the images\nBASE_DIR = \"&lt;IMAGE_DIR&gt;\"\nimage_paths = [os.path.join(BASE_DIR, image) for image in os.listdir(BASE_DIR)]\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    quantization_config=quantization_config)\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n# Load the Caption Model\ncaptioning_pipeline = Qwen2VLForImageCaptioning(\n    model = model,\n    processor = processor,\n    output_file=\"image_captioning_output.json\"\n)\n\nresults = captioning_pipeline.generate(image_paths)\n</code></pre>"},{"location":"#videos","title":"Videos","text":""},{"location":"#contributing-to-swiftannotate","title":"Contributing to SwiftAnnotate \ud83e\udd1d","text":"<p>We welcome contributions to SwiftAnnotate! There are several ways you can help improve the project:</p> <ul> <li>Enhanced Prompts: Contribute better validation and annotation prompts for improved accuracy</li> <li>File Support: Add support for additional input/output file formats</li> <li>Cloud Integration: Implement AWS S3 storage support and other cloud services</li> <li>Validation Strategies: Develop new validation approaches for different annotation tasks</li> <li>Model Support: Integrate additional LLMs and VLMs</li> <li>Documentation: Improve guides and examples</li> </ul> <p>Please submit a pull request with your contributions or open an issue to discuss new features.</p>"},{"location":"image/","title":"Image","text":""},{"location":"image/#captioning","title":"Captioning","text":""},{"location":"image/#swiftannotate.image.captioning.AutoModelForImageCaptioning","title":"<code>AutoModelForImageCaptioning</code>","text":"<p>               Bases: <code>BaseImageCaptioning</code></p> <p>AutoModelForImageCaptioning pipeline supporting different combinations of annotation and validation models OpenAI, Gemini, and Qwen2-VL.</p> <p>Example Usage: <pre><code>from swiftannotate.image import AutoModelForImageCaptioning\n\n# Initialize the pipeline\n# Note: You can use either Qwen2VL, OpenAI, and Gemini for captioning and validation.\ncaptioner = AutoModelForImageCaptioning(\n    caption_model=\"gpt-4o\",\n    validation_model=\"gemini-1.5-flash\",\n    caption_api_key=\"your_openai_api_key\",\n    validation_api_key=\"your_gemini_api_key\",\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = captioner.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         'image_path': 'path/to/image1.jpg',\n#         'image_caption': 'A cat sitting on a table.',\n#         'validation_reasoning': 'The caption is valid.',\n#         'validation_score': 0.8\n#     },\n# ]\n</code></pre></p> Source code in <code>swiftannotate/image/captioning/auto.py</code> <pre><code>class AutoModelForImageCaptioning(BaseImageCaptioning):\n    \"\"\"\n    AutoModelForImageCaptioning pipeline supporting different combinations of annotation and validation models\n    OpenAI, Gemini, and Qwen2-VL.\n\n    Example Usage:\n    ```python\n    from swiftannotate.image import AutoModelForImageCaptioning\n\n    # Initialize the pipeline\n    # Note: You can use either Qwen2VL, OpenAI, and Gemini for captioning and validation.\n    captioner = AutoModelForImageCaptioning(\n        caption_model=\"gpt-4o\",\n        validation_model=\"gemini-1.5-flash\",\n        caption_api_key=\"your_openai_api_key\",\n        validation_api_key=\"your_gemini_api_key\",\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = captioner.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         'image_path': 'path/to/image1.jpg',\n    #         'image_caption': 'A cat sitting on a table.',\n    #         'validation_reasoning': 'The caption is valid.',\n    #         'validation_score': 0.8\n    #     },\n    # ]\n    ```\n    \"\"\"\n\n    SUPPORTED_MODELS = {\n        \"openai\": [\"gpt-4o\", \"gpt-4o-mini\"],\n        \"gemini\": [\"gemini-1.5-flash\", \"gemini-1.5-pro\", \"gemini-2.0-flash-exp\", \"gemini-1.5-flash-8b\"],\n        \"local\": [Qwen2VLForConditionalGeneration]\n    }\n\n    def __init__(\n        self, \n        caption_model: str | Qwen2VLForConditionalGeneration, \n        validation_model: str | Qwen2VLForConditionalGeneration,\n        caption_model_processor: Qwen2VLProcessor | None = None,\n        validation_model_processor: Qwen2VLProcessor | None = None,\n        caption_api_key: str | None = None, \n        validation_api_key: str | None = None,\n        caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n        **kwargs\n    ):\n        \"\"\"\n        Initialize the AutoModelForImageCaptioning class.\n        This class provides functionality for automatic image captioning with optional validation.\n        It supports different combinations of annotation and validation models like OpenAI, Gemini, and Qwen2-VL.\n\n        Args:\n            caption_model (Union[str, Qwen2VLForConditionalGeneration]): \n                Model or API endpoint for caption generation.\n                Can be either a local model instance or API endpoint string.\n            validation_model (Union[str, Qwen2VLForConditionalGeneration]): \n                Model or API endpoint for caption validation.\n                Can be either a local model instance or API endpoint string.\n            caption_model_processor (Optional[Qwen2VLProcessor]): \n                Processor for caption model. \n                Required if using a local model for captioning.\n            validation_model_processor (Optional[Qwen2VLProcessor]): \n                Processor for validation model.\n                Required if using a local model for validation.\n            caption_api_key (Optional[str]): \n                API key for caption service if using API endpoint.\n            validation_api_key (Optional[str]): \n                API key for validation service if using API endpoint.\n            caption_prompt (str): \n                Prompt template for caption generation.\n                Defaults to BASE_IMAGE_CAPTION_PROMPT.\n            validation (bool): \n                Whether to perform validation on generated captions.\n                Defaults to True.\n            validation_prompt (str): \n                Prompt template for caption validation.\n                Defaults to BASE_IMAGE_CAPTION_VALIDATION_PROMPT.\n            validation_threshold (float): \n                Threshold score for caption validation.\n                Defaults to 0.5.\n            max_retry (int): \n                Maximum number of retry attempts for failed validation.\n                Defaults to 3.\n            output_file (Optional[str]): \n                Path to save results.\n                If None, results are not saved.\n            **kwargs: Additional arguments passed to model initialization.\n\n        Raises:\n            ValueError: If required model processors are not provided for local models.\n            ValueError: If an unsupported model is provided.\n\n        Note:\n            At least one of caption_model_processor or caption_api_key must be provided for caption generation.\n            Same applies for validation_model_processor or validation_api_key if validation is enabled.\n        \"\"\"\n        super().__init__(\n            caption_prompt=caption_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n        self.caption_model, self.caption_model_processor, self.caption_model_type = self._initialize_model(\n            caption_model, caption_model_processor, caption_api_key, \"caption\", **kwargs\n        )\n\n        self.validation_model, self.validation_model_processor, self.validation_model_type = self._initialize_model(\n            validation_model, validation_model_processor, validation_api_key, \"validation\", **kwargs\n        )\n\n    def _initialize_model(self, model, processor, api_key, stage, **kwargs):\n        \"\"\"Initialize model based on type.\"\"\"\n        if isinstance(model, str):\n            if model in self.SUPPORTED_MODELS[\"openai\"]:\n                self.detail = kwargs.get(\"detail\", \"low\")\n                self.client = OpenAI(api_key)\n                return model, None, \"openai\"\n            elif model in self.SUPPORTED_MODELS[\"gemini\"]:\n                return genai.GenerativeModel(model_name=model), None, \"gemini\"\n            else:\n                raise ValueError(f\"Unsupported model: {model}\")\n\n        elif isinstance(model, Qwen2VLForConditionalGeneration):\n            if processor is None:\n                raise ValueError(f\"Processor is required for Qwen2VL model in {stage} stage\")\n            self.resize_height = kwargs.get(\"resize_height\", 280)\n            self.resize_width = kwargs.get(\"resize_width\", 420)\n            return model, processor, \"qwen\"\n\n        raise ValueError(f\"Invalid model type for {stage} stage\")\n\n    def _openai_inference(self, messages: List[str], **kwargs):\n        \"\"\"Inference for OpenAI model.\"\"\"\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.caption_model,\n                messages=messages,\n                **kwargs\n            )\n            image_caption = response.choices[0].message.content.strip()\n\n        except Exception as e:\n            logger.error(f\"Image captioning failed: {e}\")\n            image_caption = \"ERROR\"\n\n        return image_caption\n\n    def _gemini_inference(self, messages: List[str], stage: str, **kwargs):\n        \"\"\"Inference for Gemini model.\"\"\"\n\n        if stage == \"annotate\":\n            try:\n                image_caption = self.caption_model.generate_content(\n                    messages,\n                    generation_config=genai.GenerationConfig(\n                        **kwargs\n                    )\n                )\n            except Exception as e:\n                logger.error(f\"Image captioning failed: {e}\")\n                image_caption = \"ERROR\"\n\n            return image_caption\n        else:\n            try:\n                validation_output = self.validation_model.generate_content(\n                    messages,\n                    generation_config=genai.GenerationConfig(\n                        response_mime_type=\"application/json\", \n                        response_schema=ImageValidationOutputGemini\n                    )\n                )\n                validation_reasoning = validation_output[\"validation_reasoning\"]\n                confidence = validation_output[\"confidence\"]\n            except Exception as e:\n                logger.error(f\"Image caption validation failed: {e}\")\n                validation_reasoning = \"ERROR\"\n                confidence = 0.0\n\n            return validation_reasoning, confidence\n\n    def _qwen_inference(self, model: Qwen2VLForConditionalGeneration, processor: Qwen2VLProcessor, messages: List[Dict], stage: str, **kwargs):\n        \"\"\"Inference for Qwen2VL model.\"\"\"\n\n        text = processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(model.device)\n\n        # Inference: Generation of the output\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 512\n\n        generated_ids = model.generate(**inputs, **kwargs)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n\n        if stage == \"annotate\":\n            image_caption = processor.batch_decode(\n                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )[0]\n\n            return image_caption\n        else:\n            validation_output = processor.batch_decode(\n                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n            )[0]\n\n            # TODO: Need a better way to parse the output\n            try:\n                validation_output = validation_output.replace('```', '').replace('json', '')\n                validation_output = json.loads(validation_output)\n                validation_reasoning = validation_output[\"validation_reasoning\"]\n                confidence = validation_output[\"confidence\"]\n            except Exception as e:\n                logger.error(f\"Image caption validation parsing failed trying to parse using another logic.\")\n\n                number_str  = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in validation_output)\n                number_str = [i for i in number_str.split() if i.isalnum()]\n                potential_confidence_scores = [float(i) for i in number_str if float(i) &gt;= 0 and float(i) &lt;= 1]\n                confidence = max(potential_confidence_scores) if potential_confidence_scores else 0.0\n                validation_reasoning = validation_output\n\n            return validation_reasoning, confidence\n\n    def annotate(self, image: str, feedback_prompt: str, **kwargs) -&gt; str:\n        \"\"\"\n        Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the caption does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated caption for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the caption you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Try to generate a better caption for the image.\n            \"\"\"\n        else:\n            user_prompt = \"Describe the given image.\"\n\n        if self.caption_model_type == \"openai\":\n            messages=[\n                {\"role\": \"system\", \"content\": self.caption_prompt},\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{image}\",\n                                \"detail\": self.detail\n                            },\n                        },\n                        {\"type\": \"text\", \"text\": user_prompt},\n                    ]\n                }\n            ]\n\n            caption = self._openai_inference(messages, \"annotate\", **kwargs)\n\n        elif self.caption_model_type == \"gemini\":\n            messages = [\n                self.caption_prompt,\n                {\"mime_type\": \"image/jpeg\", \"data\": image},\n                user_prompt,\n            ]\n\n            caption = self._gemini_inference(messages, \"annotate\", **kwargs)\n\n        else:\n            messages = [\n                {\"role\": \"system\", \"content\": self.caption_prompt},\n                {\n                    \"role\": \"user\", \n                    \"content\": [\n                        {\n                            \"type\": \"image\", \n                            \"image\": f\"data:image;base64,{image}\",\n                            \"resized_height\": self.resize_height,\n                            \"resized_width\": self.resize_width,\n                        },\n                        {\"type\": \"text\", \"text\": user_prompt},\n                    ],\n                },\n            ]\n\n            caption = self._qwen_inference(self.caption_model, self.caption_model_processor , messages, \"annotate\", **kwargs)\n\n        return caption\n\n\n    def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[bool, float]:\n        \"\"\"\n        Validates the caption generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            caption (str): Caption generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the caption.\n        \"\"\"\n        if caption == \"ERROR\":\n            return \"ERROR\", 0\n\n        if self.caption_model_type == \"openai\":\n            messages = [\n                {\n                    \"role\": \"system\",\n                    \"content\": self.validation_prompt\n                },\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{image}\",\n                                \"detail\": self.detail\n                            },\n                        },\n                        {\n                            \"type\": \"text\",\n                            \"text\": caption + \"\\nValidate the caption generated for the given image.\"\n                        }\n                    ]\n                }\n            ]  \n            validation_reasoning, confidence = self._openai_inference(messages, \"validate\", **kwargs)  \n        elif self.caption_model_type == \"gemini\":\n            messages = [\n                self.validation_prompt,\n                {'mime_type':'image/jpeg', 'data': image},\n                caption,\n                \"Validate the caption generated for the given image.\"\n            ]\n            validation_reasoning, confidence = self._gemini_inference(messages, \"validate\", **kwargs)\n        else:\n            messages = [\n                {\"role\": \"system\", \"content\": self.validation_prompt},\n                {\n                    \"role\": \"user\", \n                    \"content\": [\n                        {\n                            \"type\": \"image\", \n                            \"image\": f\"data:image;base64,{image}\",\n                            \"resized_height\": self.resize_height,\n                            \"resized_width\": self.resize_width,\n                        },\n                        {\"type\": \"text\", \"text\": caption},\n                        {\n                            \"type\": \"text\", \n                            \"text\": \"\"\"\n                            Validate the caption generated for the given image. \n                            Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                            \"\"\"\n                        },\n                    ],\n                },\n            ]\n            validation_reasoning, confidence = self._qwen_inference(self.validation_model, self.validation_model_processor, messages, \"validate\", **kwargs)\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths, **kwargs):\n        \"\"\"\n        Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n        Args:\n            image_paths (List[str]): List of image paths to generate captions for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.AutoModelForImageCaptioning.__init__","title":"<code>__init__(caption_model, validation_model, caption_model_processor=None, validation_model_processor=None, caption_api_key=None, validation_api_key=None, caption_prompt=BASE_IMAGE_CAPTION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CAPTION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None, **kwargs)</code>","text":"<p>Initialize the AutoModelForImageCaptioning class. This class provides functionality for automatic image captioning with optional validation. It supports different combinations of annotation and validation models like OpenAI, Gemini, and Qwen2-VL.</p> <p>Parameters:</p> Name Type Description Default <code>caption_model</code> <code>Union[str, Qwen2VLForConditionalGeneration]</code> <p>Model or API endpoint for caption generation. Can be either a local model instance or API endpoint string.</p> required <code>validation_model</code> <code>Union[str, Qwen2VLForConditionalGeneration]</code> <p>Model or API endpoint for caption validation. Can be either a local model instance or API endpoint string.</p> required <code>caption_model_processor</code> <code>Optional[Qwen2VLProcessor]</code> <p>Processor for caption model.  Required if using a local model for captioning.</p> <code>None</code> <code>validation_model_processor</code> <code>Optional[Qwen2VLProcessor]</code> <p>Processor for validation model. Required if using a local model for validation.</p> <code>None</code> <code>caption_api_key</code> <code>Optional[str]</code> <p>API key for caption service if using API endpoint.</p> <code>None</code> <code>validation_api_key</code> <code>Optional[str]</code> <p>API key for validation service if using API endpoint.</p> <code>None</code> <code>caption_prompt</code> <code>str</code> <p>Prompt template for caption generation. Defaults to BASE_IMAGE_CAPTION_PROMPT.</p> <code>BASE_IMAGE_CAPTION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Whether to perform validation on generated captions. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str</code> <p>Prompt template for caption validation. Defaults to BASE_IMAGE_CAPTION_VALIDATION_PROMPT.</p> <code>BASE_IMAGE_CAPTION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold score for caption validation. Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Maximum number of retry attempts for failed validation. Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>Optional[str]</code> <p>Path to save results. If None, results are not saved.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments passed to model initialization.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If required model processors are not provided for local models.</p> <code>ValueError</code> <p>If an unsupported model is provided.</p> Note <p>At least one of caption_model_processor or caption_api_key must be provided for caption generation. Same applies for validation_model_processor or validation_api_key if validation is enabled.</p> Source code in <code>swiftannotate/image/captioning/auto.py</code> <pre><code>def __init__(\n    self, \n    caption_model: str | Qwen2VLForConditionalGeneration, \n    validation_model: str | Qwen2VLForConditionalGeneration,\n    caption_model_processor: Qwen2VLProcessor | None = None,\n    validation_model_processor: Qwen2VLProcessor | None = None,\n    caption_api_key: str | None = None, \n    validation_api_key: str | None = None,\n    caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n    **kwargs\n):\n    \"\"\"\n    Initialize the AutoModelForImageCaptioning class.\n    This class provides functionality for automatic image captioning with optional validation.\n    It supports different combinations of annotation and validation models like OpenAI, Gemini, and Qwen2-VL.\n\n    Args:\n        caption_model (Union[str, Qwen2VLForConditionalGeneration]): \n            Model or API endpoint for caption generation.\n            Can be either a local model instance or API endpoint string.\n        validation_model (Union[str, Qwen2VLForConditionalGeneration]): \n            Model or API endpoint for caption validation.\n            Can be either a local model instance or API endpoint string.\n        caption_model_processor (Optional[Qwen2VLProcessor]): \n            Processor for caption model. \n            Required if using a local model for captioning.\n        validation_model_processor (Optional[Qwen2VLProcessor]): \n            Processor for validation model.\n            Required if using a local model for validation.\n        caption_api_key (Optional[str]): \n            API key for caption service if using API endpoint.\n        validation_api_key (Optional[str]): \n            API key for validation service if using API endpoint.\n        caption_prompt (str): \n            Prompt template for caption generation.\n            Defaults to BASE_IMAGE_CAPTION_PROMPT.\n        validation (bool): \n            Whether to perform validation on generated captions.\n            Defaults to True.\n        validation_prompt (str): \n            Prompt template for caption validation.\n            Defaults to BASE_IMAGE_CAPTION_VALIDATION_PROMPT.\n        validation_threshold (float): \n            Threshold score for caption validation.\n            Defaults to 0.5.\n        max_retry (int): \n            Maximum number of retry attempts for failed validation.\n            Defaults to 3.\n        output_file (Optional[str]): \n            Path to save results.\n            If None, results are not saved.\n        **kwargs: Additional arguments passed to model initialization.\n\n    Raises:\n        ValueError: If required model processors are not provided for local models.\n        ValueError: If an unsupported model is provided.\n\n    Note:\n        At least one of caption_model_processor or caption_api_key must be provided for caption generation.\n        Same applies for validation_model_processor or validation_api_key if validation is enabled.\n    \"\"\"\n    super().__init__(\n        caption_prompt=caption_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n\n    self.caption_model, self.caption_model_processor, self.caption_model_type = self._initialize_model(\n        caption_model, caption_model_processor, caption_api_key, \"caption\", **kwargs\n    )\n\n    self.validation_model, self.validation_model_processor, self.validation_model_type = self._initialize_model(\n        validation_model, validation_model_processor, validation_api_key, \"validation\", **kwargs\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.AutoModelForImageCaptioning.annotate","title":"<code>annotate(image, feedback_prompt, **kwargs)</code>","text":"<p>Annotates the image with a caption. Implements the logic to generate captions for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the caption does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better caption. Defaults to ''.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption for the image.</p> Source code in <code>swiftannotate/image/captioning/auto.py</code> <pre><code>def annotate(self, image: str, feedback_prompt: str, **kwargs) -&gt; str:\n    \"\"\"\n    Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the caption does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated caption for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the caption you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Try to generate a better caption for the image.\n        \"\"\"\n    else:\n        user_prompt = \"Describe the given image.\"\n\n    if self.caption_model_type == \"openai\":\n        messages=[\n            {\"role\": \"system\", \"content\": self.caption_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ]\n            }\n        ]\n\n        caption = self._openai_inference(messages, \"annotate\", **kwargs)\n\n    elif self.caption_model_type == \"gemini\":\n        messages = [\n            self.caption_prompt,\n            {\"mime_type\": \"image/jpeg\", \"data\": image},\n            user_prompt,\n        ]\n\n        caption = self._gemini_inference(messages, \"annotate\", **kwargs)\n\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": self.caption_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ],\n            },\n        ]\n\n        caption = self._qwen_inference(self.caption_model, self.caption_model_processor , messages, \"annotate\", **kwargs)\n\n    return caption\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.AutoModelForImageCaptioning.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates captions for a list of images. Implements the logic to generate captions for a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate captions for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <p>List[Dict]: List of captions, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/captioning/auto.py</code> <pre><code>def generate(self, image_paths, **kwargs):\n    \"\"\"\n    Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n    Args:\n        image_paths (List[str]): List of image paths to generate captions for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.AutoModelForImageCaptioning.validate","title":"<code>validate(image, caption, **kwargs)</code>","text":"<p>Validates the caption generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>caption</code> <code>str</code> <p>Caption generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[bool, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the caption.</p> Source code in <code>swiftannotate/image/captioning/auto.py</code> <pre><code>def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[bool, float]:\n    \"\"\"\n    Validates the caption generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        caption (str): Caption generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the caption.\n    \"\"\"\n    if caption == \"ERROR\":\n        return \"ERROR\", 0\n\n    if self.caption_model_type == \"openai\":\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.validation_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": caption + \"\\nValidate the caption generated for the given image.\"\n                    }\n                ]\n            }\n        ]  \n        validation_reasoning, confidence = self._openai_inference(messages, \"validate\", **kwargs)  \n    elif self.caption_model_type == \"gemini\":\n        messages = [\n            self.validation_prompt,\n            {'mime_type':'image/jpeg', 'data': image},\n            caption,\n            \"Validate the caption generated for the given image.\"\n        ]\n        validation_reasoning, confidence = self._gemini_inference(messages, \"validate\", **kwargs)\n    else:\n        messages = [\n            {\"role\": \"system\", \"content\": self.validation_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": caption},\n                    {\n                        \"type\": \"text\", \n                        \"text\": \"\"\"\n                        Validate the caption generated for the given image. \n                        Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                        \"\"\"\n                    },\n                ],\n            },\n        ]\n        validation_reasoning, confidence = self._qwen_inference(self.validation_model, self.validation_model_processor, messages, \"validate\", **kwargs)\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.GeminiForImageCaptioning","title":"<code>GeminiForImageCaptioning</code>","text":"<p>               Bases: <code>BaseImageCaptioning</code></p> <p>GeminiForImageClassification pipeline for generating captions for images using Gemini models.</p> <p>Example usage: <pre><code>from swiftannotate.image import GeminiForImageCaptioning\n\n# Initialize the pipeline\ncaptioner = GeminiForImageCaptioning(\n    caption_model=\"gemini-1.5-pro\",\n    validation_model=\"gemini-1.5-flash\",\n    api_key=\"your_api_key_here\",\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = captioner.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         'image_path': 'path/to/image1.jpg', \n#         'image_caption': 'A cat sitting on a table.', \n#         'validation_reasoning': 'The caption is valid.', \n#         'validation_score': 0.8\n#     }, \n# ]\n</code></pre></p> Source code in <code>swiftannotate/image/captioning/gemini.py</code> <pre><code>class GeminiForImageCaptioning(BaseImageCaptioning):\n    \"\"\"\n    GeminiForImageClassification pipeline for generating captions for images using Gemini models.\n\n    Example usage:\n    ```python\n    from swiftannotate.image import GeminiForImageCaptioning\n\n    # Initialize the pipeline\n    captioner = GeminiForImageCaptioning(\n        caption_model=\"gemini-1.5-pro\",\n        validation_model=\"gemini-1.5-flash\",\n        api_key=\"your_api_key_here\",\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = captioner.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         'image_path': 'path/to/image1.jpg', \n    #         'image_caption': 'A cat sitting on a table.', \n    #         'validation_reasoning': 'The caption is valid.', \n    #         'validation_score': 0.8\n    #     }, \n    # ]\n    ```\n    \"\"\"\n\n    def __init__(\n        self, \n        caption_model: str, \n        validation_model: str,\n        api_key: str, \n        caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n    ):\n        \"\"\"\n        Initializes the ImageCaptioningGemini pipeline.\n\n        Args:\n            caption_model (str): \n                Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n            validation_model (str): \n                Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n            api_key (str): \n                Google Gemini API key.\n            caption_prompt (str | None, optional): \n                System prompt for captioning images.\n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image captions should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image caption is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image caption. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n        \"\"\"        \n        genai.configure(api_key=api_key)\n        self.caption_model = genai.GenerativeModel(model=caption_model)\n        self.validation_model = genai.GenerativeModel(model=validation_model)\n\n        super().__init__(\n            caption_prompt=caption_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n        \"\"\"\n        Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the caption does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated caption for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the caption you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Try to generate a better caption for the image.\n            \"\"\"\n        else:\n            user_prompt = \"Describe the given image.\"\n\n        messages = [\n            self.caption_prompt,\n            {'mime_type':'image/jpeg', 'data': image}, \n            user_prompt\n        ]\n\n        try:\n            image_caption = self.caption_model.generate_content(\n                messages,\n                generation_config=genai.GenerationConfig(\n                    **kwargs\n                )\n            )\n        except Exception as e:\n            logger.error(f\"Image captioning failed: {e}\")\n            image_caption = \"ERROR\"\n\n        return image_caption\n\n    def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]:\n        \"\"\"\n        Validates the caption generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            caption (str): Caption generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the caption.\n        \"\"\"\n        if caption == \"ERROR\":\n            return \"ERROR\", 0.0\n\n        messages = [\n            self.validation_prompt,\n            {'mime_type':'image/jpeg', 'data': image},\n            caption,\n            \"Validate the caption generated for the given image.\"\n        ]\n\n        try:\n            validation_output = self.validation_model.generate_content(\n                messages,\n                generation_config=genai.GenerationConfig(\n                    response_mime_type=\"application/json\", \n                    response_schema=ImageValidationOutputGemini\n                )\n            )\n            validation_reasoning = validation_output[\"validation_reasoning\"]\n            confidence = validation_output[\"confidence\"]\n        except Exception as e:\n            logger.error(f\"Image caption validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0.0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n        Args:\n            image_paths (List[str]): List of image paths to generate captions for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n        \"\"\"\n\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results \n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.GeminiForImageCaptioning.__init__","title":"<code>__init__(caption_model, validation_model, api_key, caption_prompt=BASE_IMAGE_CAPTION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CAPTION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None)</code>","text":"<p>Initializes the ImageCaptioningGemini pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>caption_model</code> <code>str</code> <p>Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.</p> required <code>validation_model</code> <code>str</code> <p>Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.</p> required <code>api_key</code> <code>str</code> <p>Google Gemini API key.</p> required <code>caption_prompt</code> <code>str | None</code> <p>System prompt for captioning images. Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image captions should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image caption is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image caption.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> Notes <p><code>validation_prompt</code> should specify the rules for validating the caption and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> Source code in <code>swiftannotate/image/captioning/gemini.py</code> <pre><code>def __init__(\n    self, \n    caption_model: str, \n    validation_model: str,\n    api_key: str, \n    caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n):\n    \"\"\"\n    Initializes the ImageCaptioningGemini pipeline.\n\n    Args:\n        caption_model (str): \n            Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n        validation_model (str): \n            Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n        api_key (str): \n            Google Gemini API key.\n        caption_prompt (str | None, optional): \n            System prompt for captioning images.\n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image captions should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image caption is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image caption. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n    \"\"\"        \n    genai.configure(api_key=api_key)\n    self.caption_model = genai.GenerativeModel(model=caption_model)\n    self.validation_model = genai.GenerativeModel(model=validation_model)\n\n    super().__init__(\n        caption_prompt=caption_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.GeminiForImageCaptioning.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a caption. Implements the logic to generate captions for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the caption does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better caption. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption for the image.</p> Source code in <code>swiftannotate/image/captioning/gemini.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n    \"\"\"\n    Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the caption does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated caption for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the caption you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Try to generate a better caption for the image.\n        \"\"\"\n    else:\n        user_prompt = \"Describe the given image.\"\n\n    messages = [\n        self.caption_prompt,\n        {'mime_type':'image/jpeg', 'data': image}, \n        user_prompt\n    ]\n\n    try:\n        image_caption = self.caption_model.generate_content(\n            messages,\n            generation_config=genai.GenerationConfig(\n                **kwargs\n            )\n        )\n    except Exception as e:\n        logger.error(f\"Image captioning failed: {e}\")\n        image_caption = \"ERROR\"\n\n    return image_caption\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.GeminiForImageCaptioning.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates captions for a list of images. Implements the logic to generate captions for a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate captions for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of captions, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/captioning/gemini.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n    Args:\n        image_paths (List[str]): List of image paths to generate captions for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n    \"\"\"\n\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results \n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.GeminiForImageCaptioning.validate","title":"<code>validate(image, caption, **kwargs)</code>","text":"<p>Validates the caption generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>caption</code> <code>str</code> <p>Caption generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the caption.</p> Source code in <code>swiftannotate/image/captioning/gemini.py</code> <pre><code>def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]:\n    \"\"\"\n    Validates the caption generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        caption (str): Caption generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the caption.\n    \"\"\"\n    if caption == \"ERROR\":\n        return \"ERROR\", 0.0\n\n    messages = [\n        self.validation_prompt,\n        {'mime_type':'image/jpeg', 'data': image},\n        caption,\n        \"Validate the caption generated for the given image.\"\n    ]\n\n    try:\n        validation_output = self.validation_model.generate_content(\n            messages,\n            generation_config=genai.GenerationConfig(\n                response_mime_type=\"application/json\", \n                response_schema=ImageValidationOutputGemini\n            )\n        )\n        validation_reasoning = validation_output[\"validation_reasoning\"]\n        confidence = validation_output[\"confidence\"]\n    except Exception as e:\n        logger.error(f\"Image caption validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0.0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OllamaForImageCaptioning","title":"<code>OllamaForImageCaptioning</code>","text":"<p>               Bases: <code>BaseImageCaptioning</code></p> <p>OllamaForImageCaptioning pipeline using Ollama API.</p> <p>Example usage:</p> <pre><code>from swiftannotate.image import OllamaForImageCaptioning\n\n# Initialize the pipeline\ncaptioner = OllamaForImageCaptioning(\n    caption_model=\"llama3.2-vision\",\n    validation_model=\"llama3.2-vision\",\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = captioner.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         'image_path': 'path/to/image1.jpg', \n#         'image_caption': 'A cat sitting on a table.', \n#         'validation_reasoning': 'The caption is valid.', \n#         'validation_score': 0.8\n#     }, \n# ]\n</code></pre> Source code in <code>swiftannotate/image/captioning/ollama.py</code> <pre><code>class OllamaForImageCaptioning(BaseImageCaptioning):\n    \"\"\"\n    OllamaForImageCaptioning pipeline using Ollama API.\n\n    Example usage:\n\n    ```python\n    from swiftannotate.image import OllamaForImageCaptioning\n\n    # Initialize the pipeline\n    captioner = OllamaForImageCaptioning(\n        caption_model=\"llama3.2-vision\",\n        validation_model=\"llama3.2-vision\",\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = captioner.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         'image_path': 'path/to/image1.jpg', \n    #         'image_caption': 'A cat sitting on a table.', \n    #         'validation_reasoning': 'The caption is valid.', \n    #         'validation_score': 0.8\n    #     }, \n    # ]\n    ```\n    \"\"\"\n\n    def __init__(\n        self, \n        caption_model: str, \n        validation_model: str,\n        caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n    ):\n        \"\"\"\n        Initializes the OllamaForImageCaptioning pipeline.\n\n        Args:\n            caption_model (str): \n                Can be either any of the Multimodal (Vision) models supported by Ollama.\n                specific versions of model supported by Ollama.\n            validation_model (str): \n                Can be either any of the Multimodal (Vision) models supported by Ollama.\n                specific versions of model supported by Ollama.\n            caption_prompt (str | None, optional): \n                System prompt for captioning images.\n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image captions should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image caption is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image caption. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n        \"\"\"\n\n        if not self._validate_ollama_model(caption_model):\n            raise ValueError(f\"Caption model {caption_model} is not supported by Ollama.\")\n\n        if not self._validate_ollama_model(validation_model):\n            raise ValueError(f\"Validation model {validation_model} is not supported by Ollama.\")\n\n        self.caption_model = caption_model\n        self.validation_model = validation_model\n\n        super().__init__(\n            caption_prompt=caption_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n    def _validate_ollama_model(self, model: str) -&gt; bool:\n        try:\n            ollama.chat(model)\n        except ollama.ResponseError as e:\n            logger.error(f\"Error: {e.error}\")\n            if e.status_code == 404:\n                try:\n                    ollama.pull(model)\n                    logger.info(f\"Model {model} is now downloaded.\")\n                except ollama.ResponseError as e:\n                    logger.error(f\"Error: {e.error}\")\n                    logger.error(f\"Model {model} could not be downloaded. Check the model name and try again.\")\n                    return False\n            logger.info(f\"Model {model} is now downloaded.\")\n\n        return True\n\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n        \"\"\"\n        Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the caption does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated caption for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the caption you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Try to generate a better caption for the image.\n            \"\"\"\n        else:\n            user_prompt = \"Describe the given image.\"\n\n        messages=[\n            {\"role\": \"system\", \"content\": self.caption_prompt},\n            {\n                \"role\": \"user\",\n                \"images\": [image],\n                \"content\": user_prompt\n            }\n        ]\n\n        if not \"temperature\" in kwargs:\n            kwargs[\"temperature\"] = 0.0\n\n        try:\n\n            response = ollama.chat(\n                model=self.caption_model,\n                messages=messages,\n                options=kwargs\n            )\n            image_caption = response.message.content\n\n        except Exception as e:\n            logger.error(f\"Image captioning failed: {e}\")\n            image_caption = \"ERROR\"\n\n        return image_caption\n\n    def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]: \n        \"\"\"\n        Validates the caption generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            caption (str): Caption generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the caption.\n        \"\"\"\n        if caption == \"ERROR\":\n            return \"ERROR\", 0\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.validation_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"images\": [image],\n                \"content\": caption + \"\\nValidate the caption generated for the given image.\"\n            }\n        ]      \n\n        if not \"temperature\" in kwargs:\n            kwargs[\"temperature\"] = 0.0\n\n        try:\n\n            response = ollama.chat(\n                model=self.validation_model,\n                messages=messages,\n                format=ImageValidationOutputOllama.model_json_schema(),\n                options=kwargs\n            )\n\n            validation_output = ImageValidationOutputOllama.model_validate_json(response.message.content)\n\n            validation_reasoning = validation_output.validation_reasoning\n            confidence = validation_output.confidence\n\n        except Exception as e:\n            logger.error(f\"Image caption validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n        Args:\n            image_paths (List[str]): List of image paths to generate captions for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OllamaForImageCaptioning.__init__","title":"<code>__init__(caption_model, validation_model, caption_prompt=BASE_IMAGE_CAPTION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CAPTION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None)</code>","text":"<p>Initializes the OllamaForImageCaptioning pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>caption_model</code> <code>str</code> <p>Can be either any of the Multimodal (Vision) models supported by Ollama. specific versions of model supported by Ollama.</p> required <code>validation_model</code> <code>str</code> <p>Can be either any of the Multimodal (Vision) models supported by Ollama. specific versions of model supported by Ollama.</p> required <code>caption_prompt</code> <code>str | None</code> <p>System prompt for captioning images. Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image captions should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image caption is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image caption.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> Notes <p><code>validation_prompt</code> should specify the rules for validating the caption and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> Source code in <code>swiftannotate/image/captioning/ollama.py</code> <pre><code>def __init__(\n    self, \n    caption_model: str, \n    validation_model: str,\n    caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n):\n    \"\"\"\n    Initializes the OllamaForImageCaptioning pipeline.\n\n    Args:\n        caption_model (str): \n            Can be either any of the Multimodal (Vision) models supported by Ollama.\n            specific versions of model supported by Ollama.\n        validation_model (str): \n            Can be either any of the Multimodal (Vision) models supported by Ollama.\n            specific versions of model supported by Ollama.\n        caption_prompt (str | None, optional): \n            System prompt for captioning images.\n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image captions should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image caption is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image caption. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n    \"\"\"\n\n    if not self._validate_ollama_model(caption_model):\n        raise ValueError(f\"Caption model {caption_model} is not supported by Ollama.\")\n\n    if not self._validate_ollama_model(validation_model):\n        raise ValueError(f\"Validation model {validation_model} is not supported by Ollama.\")\n\n    self.caption_model = caption_model\n    self.validation_model = validation_model\n\n    super().__init__(\n        caption_prompt=caption_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OllamaForImageCaptioning.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a caption. Implements the logic to generate captions for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the caption does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better caption. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption for the image.</p> Source code in <code>swiftannotate/image/captioning/ollama.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n    \"\"\"\n    Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the caption does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated caption for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the caption you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Try to generate a better caption for the image.\n        \"\"\"\n    else:\n        user_prompt = \"Describe the given image.\"\n\n    messages=[\n        {\"role\": \"system\", \"content\": self.caption_prompt},\n        {\n            \"role\": \"user\",\n            \"images\": [image],\n            \"content\": user_prompt\n        }\n    ]\n\n    if not \"temperature\" in kwargs:\n        kwargs[\"temperature\"] = 0.0\n\n    try:\n\n        response = ollama.chat(\n            model=self.caption_model,\n            messages=messages,\n            options=kwargs\n        )\n        image_caption = response.message.content\n\n    except Exception as e:\n        logger.error(f\"Image captioning failed: {e}\")\n        image_caption = \"ERROR\"\n\n    return image_caption\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OllamaForImageCaptioning.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates captions for a list of images. Implements the logic to generate captions for a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate captions for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of captions, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/captioning/ollama.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n    Args:\n        image_paths (List[str]): List of image paths to generate captions for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OllamaForImageCaptioning.validate","title":"<code>validate(image, caption, **kwargs)</code>","text":"<p>Validates the caption generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>caption</code> <code>str</code> <p>Caption generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the caption.</p> Source code in <code>swiftannotate/image/captioning/ollama.py</code> <pre><code>def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]: \n    \"\"\"\n    Validates the caption generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        caption (str): Caption generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the caption.\n    \"\"\"\n    if caption == \"ERROR\":\n        return \"ERROR\", 0\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": self.validation_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"images\": [image],\n            \"content\": caption + \"\\nValidate the caption generated for the given image.\"\n        }\n    ]      \n\n    if not \"temperature\" in kwargs:\n        kwargs[\"temperature\"] = 0.0\n\n    try:\n\n        response = ollama.chat(\n            model=self.validation_model,\n            messages=messages,\n            format=ImageValidationOutputOllama.model_json_schema(),\n            options=kwargs\n        )\n\n        validation_output = ImageValidationOutputOllama.model_validate_json(response.message.content)\n\n        validation_reasoning = validation_output.validation_reasoning\n        confidence = validation_output.confidence\n\n    except Exception as e:\n        logger.error(f\"Image caption validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OpenAIForImageCaptioning","title":"<code>OpenAIForImageCaptioning</code>","text":"<p>               Bases: <code>BaseImageCaptioning</code></p> <p>OpenAIForImageCaptioning pipeline using OpenAI API.</p> <p>Example usage:</p> <pre><code>from swiftannotate.image import OpenAIForImageCaptioning\n\n# Initialize the pipeline\ncaptioner = OpenAIForImageCaptioning(\n    caption_model=\"gpt-4o\",\n    validation_model=\"gpt-4o-mini\",\n    api_key=\"your_api_key_here\",\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = captioner.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         'image_path': 'path/to/image1.jpg', \n#         'image_caption': 'A cat sitting on a table.', \n#         'validation_reasoning': 'The caption is valid.', \n#         'validation_score': 0.8\n#     }, \n# ]\n</code></pre> Source code in <code>swiftannotate/image/captioning/openai.py</code> <pre><code>class OpenAIForImageCaptioning(BaseImageCaptioning):\n    \"\"\"\n    OpenAIForImageCaptioning pipeline using OpenAI API.\n\n    Example usage:\n\n    ```python\n    from swiftannotate.image import OpenAIForImageCaptioning\n\n    # Initialize the pipeline\n    captioner = OpenAIForImageCaptioning(\n        caption_model=\"gpt-4o\",\n        validation_model=\"gpt-4o-mini\",\n        api_key=\"your_api_key_here\",\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = captioner.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         'image_path': 'path/to/image1.jpg', \n    #         'image_caption': 'A cat sitting on a table.', \n    #         'validation_reasoning': 'The caption is valid.', \n    #         'validation_score': 0.8\n    #     }, \n    # ]\n    ```\n    \"\"\"\n\n    def __init__(\n        self, \n        caption_model: str, \n        validation_model: str,\n        api_key: str, \n        caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n        **kwargs\n    ):\n        \"\"\"\n        Initializes the ImageCaptioningOpenAI pipeline.\n\n        Args:\n            caption_model (str): \n                Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n                specific versions of model supported by OpenAI.\n            validation_model (str): \n                Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n                specific versions of model supported by OpenAI.\n            api_key (str): OpenAI API key.\n            caption_prompt (str | None, optional): \n                System prompt for captioning images.\n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image captions should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image caption is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image caption. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Keyword Arguments:\n            detail (str, optional): \n                Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n        \"\"\"\n        self.caption_model = caption_model\n        self.validation_model = validation_model\n        self.client = OpenAI(api_key)\n\n        super().__init__(\n            caption_prompt=caption_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n        self.detail = kwargs.get(\"detail\", \"low\")\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n        \"\"\"\n        Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the caption does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated caption for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the caption you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Try to generate a better caption for the image.\n            \"\"\"\n        else:\n            user_prompt = \"Describe the given image.\"\n\n        messages=[\n            {\"role\": \"system\", \"content\": self.caption_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ]\n            }\n        ]\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.caption_model,\n                messages=messages,\n                **kwargs\n            )\n            image_caption = response.choices[0].message.content.strip()\n\n        except Exception as e:\n            logger.error(f\"Image captioning failed: {e}\")\n            image_caption = \"ERROR\"\n\n        return image_caption\n\n    def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]: \n        \"\"\"\n        Validates the caption generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            caption (str): Caption generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the caption.\n        \"\"\"\n        if caption == \"ERROR\":\n            return \"ERROR\", 0\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.validation_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": caption + \"\\nValidate the caption generated for the given image.\"\n                    }\n                ]\n            }\n        ]      \n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.validation_model,\n                messages=messages,\n                response_format=ImageValidationOutputOpenAI,\n                **kwargs\n            )\n            validation_output = response.choices[0].message.parsed\n            validation_reasoning = validation_output.validation_reasoning\n            confidence = validation_output.confidence\n\n        except Exception as e:\n            logger.error(f\"Image caption validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n        Args:\n            image_paths (List[str]): List of image paths to generate captions for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OpenAIForImageCaptioning.__init__","title":"<code>__init__(caption_model, validation_model, api_key, caption_prompt=BASE_IMAGE_CAPTION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CAPTION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None, **kwargs)</code>","text":"<p>Initializes the ImageCaptioningOpenAI pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>caption_model</code> <code>str</code> <p>Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or  specific versions of model supported by OpenAI.</p> required <code>validation_model</code> <code>str</code> <p>Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or  specific versions of model supported by OpenAI.</p> required <code>api_key</code> <code>str</code> <p>OpenAI API key.</p> required <code>caption_prompt</code> <code>str | None</code> <p>System prompt for captioning images. Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image captions should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image caption is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image caption.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>detail</code> <code>str</code> <p>Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".</p> Notes <p><code>validation_prompt</code> should specify the rules for validating the caption and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> Source code in <code>swiftannotate/image/captioning/openai.py</code> <pre><code>def __init__(\n    self, \n    caption_model: str, \n    validation_model: str,\n    api_key: str, \n    caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n    **kwargs\n):\n    \"\"\"\n    Initializes the ImageCaptioningOpenAI pipeline.\n\n    Args:\n        caption_model (str): \n            Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n            specific versions of model supported by OpenAI.\n        validation_model (str): \n            Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n            specific versions of model supported by OpenAI.\n        api_key (str): OpenAI API key.\n        caption_prompt (str | None, optional): \n            System prompt for captioning images.\n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image captions should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image caption is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image caption. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Keyword Arguments:\n        detail (str, optional): \n            Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n    \"\"\"\n    self.caption_model = caption_model\n    self.validation_model = validation_model\n    self.client = OpenAI(api_key)\n\n    super().__init__(\n        caption_prompt=caption_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n\n    self.detail = kwargs.get(\"detail\", \"low\")\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OpenAIForImageCaptioning.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a caption. Implements the logic to generate captions for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the caption does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better caption. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption for the image.</p> Source code in <code>swiftannotate/image/captioning/openai.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n    \"\"\"\n    Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the caption does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated caption for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the caption you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Try to generate a better caption for the image.\n        \"\"\"\n    else:\n        user_prompt = \"Describe the given image.\"\n\n    messages=[\n        {\"role\": \"system\", \"content\": self.caption_prompt},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\n                        \"detail\": self.detail\n                    },\n                },\n                {\"type\": \"text\", \"text\": user_prompt},\n            ]\n        }\n    ]\n\n    try:\n        response = self.client.chat.completions.create(\n            model=self.caption_model,\n            messages=messages,\n            **kwargs\n        )\n        image_caption = response.choices[0].message.content.strip()\n\n    except Exception as e:\n        logger.error(f\"Image captioning failed: {e}\")\n        image_caption = \"ERROR\"\n\n    return image_caption\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OpenAIForImageCaptioning.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates captions for a list of images. Implements the logic to generate captions for a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate captions for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of captions, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/captioning/openai.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n    Args:\n        image_paths (List[str]): List of image paths to generate captions for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.OpenAIForImageCaptioning.validate","title":"<code>validate(image, caption, **kwargs)</code>","text":"<p>Validates the caption generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>caption</code> <code>str</code> <p>Caption generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the caption.</p> Source code in <code>swiftannotate/image/captioning/openai.py</code> <pre><code>def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]: \n    \"\"\"\n    Validates the caption generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        caption (str): Caption generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the caption.\n    \"\"\"\n    if caption == \"ERROR\":\n        return \"ERROR\", 0\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": self.validation_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\n                        \"detail\": self.detail\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": caption + \"\\nValidate the caption generated for the given image.\"\n                }\n            ]\n        }\n    ]      \n\n    try:\n        response = self.client.chat.completions.create(\n            model=self.validation_model,\n            messages=messages,\n            response_format=ImageValidationOutputOpenAI,\n            **kwargs\n        )\n        validation_output = response.choices[0].message.parsed\n        validation_reasoning = validation_output.validation_reasoning\n        confidence = validation_output.confidence\n\n    except Exception as e:\n        logger.error(f\"Image caption validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.Qwen2VLForImageCaptioning","title":"<code>Qwen2VLForImageCaptioning</code>","text":"<p>               Bases: <code>BaseImageCaptioning</code></p> <p>Qwen2VLForImageCaptioning pipeline using Qwen2VL model.</p> <p>Example usage: <pre><code>from transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers import BitsAndBytesConfig\nfrom swiftannotate.image import Qwen2VLForImageCaptioning\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    quantization_config=quantization_config)\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n# Load the Caption Model\ncaptioning_pipeline = Qwen2VLForImageCaptioning(\n    model = model,\n    processor = processor,\n    output_file=\"captions.json\"\n)\n\n# Generate captions for images\nimage_paths = ['path/to/image1.jpg']\nresults = captioning_pipeline.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         'image_path': 'path/to/image1.jpg', \n#         'image_caption': 'A cat sitting on a table.', \n#         'validation_reasoning': 'The caption is valid.', \n#         'validation_score': 0.8\n#     }, \n# ]\n</code></pre></p> Source code in <code>swiftannotate/image/captioning/qwen.py</code> <pre><code>class Qwen2VLForImageCaptioning(BaseImageCaptioning):\n    \"\"\"\n    Qwen2VLForImageCaptioning pipeline using Qwen2VL model.\n\n    Example usage:\n    ```python\n    from transformers import AutoProcessor, AutoModelForImageTextToText\n    from transformers import BitsAndBytesConfig\n    from swiftannotate.image import Qwen2VLForImageCaptioning\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=\"float16\",\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForImageTextToText.from_pretrained(\n        \"Qwen/Qwen2-VL-7B-Instruct\",\n        device_map=\"auto\",\n        torch_dtype=\"auto\",\n        quantization_config=quantization_config)\n\n    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n    # Load the Caption Model\n    captioning_pipeline = Qwen2VLForImageCaptioning(\n        model = model,\n        processor = processor,\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for images\n    image_paths = ['path/to/image1.jpg']\n    results = captioning_pipeline.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         'image_path': 'path/to/image1.jpg', \n    #         'image_caption': 'A cat sitting on a table.', \n    #         'validation_reasoning': 'The caption is valid.', \n    #         'validation_score': 0.8\n    #     }, \n    # ]\n    ```\n    \"\"\"\n    def __init__(\n        self, \n        model: AutoModelForImageTextToText | Qwen2VLForConditionalGeneration, \n        processor: AutoProcessor | Qwen2VLProcessor,\n        caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n        **kwargs\n    ):     \n        \"\"\"\n        Initializes the ImageCaptioningQwen2VL pipeline.\n\n        Args:\n            model (AutoModelForImageTextToText): \n                Model for image captioning. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights.\n                Can be any version of Qwen2-VL model (7B, 72B).\n            processor (AutoProcessor): \n                Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.\n            caption_prompt (str | None, optional): \n                System prompt for captioning images.\n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image captions should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image caption is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image caption. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Keyword Arguments:\n            resize_height (int, optional):\n                Height to resize the image before generating captions. Defaults to 280.\n            resize_width (int, optional):\n                Width to resize the image before generating captions. Defaults to 420.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n        \"\"\"    \n\n        if not isinstance(model, Qwen2VLForConditionalGeneration):\n            raise ValueError(\"Model should be an instance of Qwen2VLForConditionalGeneration.\")\n        if not isinstance(processor, Qwen2VLProcessor):\n            raise ValueError(\"Processor should be an instance of Qwen2VLProcessor.\")\n\n        self.model = model\n        self.processor = processor\n\n        super().__init__(\n            caption_prompt=caption_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n        self.resize_height = kwargs.get(\"resize_height\", 280)\n        self.resize_width = kwargs.get(\"resize_width\", 420)\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n        \"\"\"\n        Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the caption does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated caption for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the caption you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Try to generate a better caption for the image.\n            \"\"\"\n        else:\n            user_prompt = \"Describe the given image.\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": self.caption_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ],\n            },\n        ]\n\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(self.model.device)\n\n        # Inference: Generation of the output\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 512\n\n        generated_ids = self.model.generate(**inputs, **kwargs)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        image_caption = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        return image_caption\n\n    def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]:\n        \"\"\"\n        Validates the caption generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            caption (str): Caption generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the caption.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.validation_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": caption},\n                    {\n                        \"type\": \"text\", \n                        \"text\": \"\"\"\n                        Validate the caption generated for the given image. \n                        Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                        \"\"\"\n                    },\n                ],\n            },\n        ]\n\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(self.model.device)\n\n        # Inference: Generation of the output\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 512\n\n        generated_ids = self.model.generate(**inputs, **kwargs)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        validation_output = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        # TODO: Need a better way to parse the output\n        try:\n            validation_output = validation_output.replace('```', '').replace('json', '')\n            validation_output = json.loads(validation_output)\n            validation_reasoning = validation_output[\"validation_reasoning\"]\n            confidence = validation_output[\"confidence\"]\n        except Exception as e:\n            logger.error(f\"Image caption validation parsing failed trying to parse using another logic.\")\n\n            number_str  = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in validation_output)\n            number_str = [i for i in number_str.split() if i.isalnum()]\n            potential_confidence_scores = [float(i) for i in number_str if float(i) &gt;= 0 and float(i) &lt;= 1]\n            confidence = max(potential_confidence_scores) if potential_confidence_scores else 0.0\n            validation_reasoning = validation_output\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n        Args:\n            image_paths (List[str]): List of image paths to generate captions for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.Qwen2VLForImageCaptioning.__init__","title":"<code>__init__(model, processor, caption_prompt=BASE_IMAGE_CAPTION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CAPTION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None, **kwargs)</code>","text":"<p>Initializes the ImageCaptioningQwen2VL pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AutoModelForImageTextToText</code> <p>Model for image captioning. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights. Can be any version of Qwen2-VL model (7B, 72B).</p> required <code>processor</code> <code>AutoProcessor</code> <p>Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.</p> required <code>caption_prompt</code> <code>str | None</code> <p>System prompt for captioning images. Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image captions should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CAPTION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image caption is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image caption.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>resize_height</code> <code>int</code> <p>Height to resize the image before generating captions. Defaults to 280.</p> <code>resize_width</code> <code>int</code> <p>Width to resize the image before generating captions. Defaults to 420.</p> Notes <p><code>validation_prompt</code> should specify the rules for validating the caption and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> Source code in <code>swiftannotate/image/captioning/qwen.py</code> <pre><code>def __init__(\n    self, \n    model: AutoModelForImageTextToText | Qwen2VLForConditionalGeneration, \n    processor: AutoProcessor | Qwen2VLProcessor,\n    caption_prompt: str = BASE_IMAGE_CAPTION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CAPTION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n    **kwargs\n):     \n    \"\"\"\n    Initializes the ImageCaptioningQwen2VL pipeline.\n\n    Args:\n        model (AutoModelForImageTextToText): \n            Model for image captioning. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights.\n            Can be any version of Qwen2-VL model (7B, 72B).\n        processor (AutoProcessor): \n            Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.\n        caption_prompt (str | None, optional): \n            System prompt for captioning images.\n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image captions should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CAPTION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image caption is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image caption. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Keyword Arguments:\n        resize_height (int, optional):\n            Height to resize the image before generating captions. Defaults to 280.\n        resize_width (int, optional):\n            Width to resize the image before generating captions. Defaults to 420.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the caption and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n    \"\"\"    \n\n    if not isinstance(model, Qwen2VLForConditionalGeneration):\n        raise ValueError(\"Model should be an instance of Qwen2VLForConditionalGeneration.\")\n    if not isinstance(processor, Qwen2VLProcessor):\n        raise ValueError(\"Processor should be an instance of Qwen2VLProcessor.\")\n\n    self.model = model\n    self.processor = processor\n\n    super().__init__(\n        caption_prompt=caption_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n\n    self.resize_height = kwargs.get(\"resize_height\", 280)\n    self.resize_width = kwargs.get(\"resize_width\", 420)\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.Qwen2VLForImageCaptioning.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a caption. Implements the logic to generate captions for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the caption does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better caption. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated caption for the image.</p> Source code in <code>swiftannotate/image/captioning/qwen.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n    \"\"\"\n    Annotates the image with a caption. Implements the logic to generate captions for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the caption does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better caption. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated caption for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the caption you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Try to generate a better caption for the image.\n        \"\"\"\n    else:\n        user_prompt = \"Describe the given image.\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": self.caption_prompt},\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"data:image;base64,{image}\",\n                    \"resized_height\": self.resize_height,\n                    \"resized_width\": self.resize_width,\n                },\n                {\"type\": \"text\", \"text\": user_prompt},\n            ],\n        },\n    ]\n\n    text = self.processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = self.processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(self.model.device)\n\n    # Inference: Generation of the output\n    if \"max_new_tokens\" not in kwargs:\n        kwargs[\"max_new_tokens\"] = 512\n\n    generated_ids = self.model.generate(**inputs, **kwargs)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    image_caption = self.processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n\n    return image_caption\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.Qwen2VLForImageCaptioning.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates captions for a list of images. Implements the logic to generate captions for a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate captions for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of captions, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/captioning/qwen.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates captions for a list of images. Implements the logic to generate captions for a list of images.\n\n    Args:\n        image_paths (List[str]): List of image paths to generate captions for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of captions, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.captioning.Qwen2VLForImageCaptioning.validate","title":"<code>validate(image, caption, **kwargs)</code>","text":"<p>Validates the caption generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>caption</code> <code>str</code> <p>Caption generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the caption.</p> Source code in <code>swiftannotate/image/captioning/qwen.py</code> <pre><code>def validate(self, image: str, caption: str, **kwargs) -&gt; Tuple[str, float]:\n    \"\"\"\n    Validates the caption generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        caption (str): Caption generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the caption.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.validation_prompt},\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"data:image;base64,{image}\",\n                    \"resized_height\": self.resize_height,\n                    \"resized_width\": self.resize_width,\n                },\n                {\"type\": \"text\", \"text\": caption},\n                {\n                    \"type\": \"text\", \n                    \"text\": \"\"\"\n                    Validate the caption generated for the given image. \n                    Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                    \"\"\"\n                },\n            ],\n        },\n    ]\n\n    text = self.processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = self.processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(self.model.device)\n\n    # Inference: Generation of the output\n    if \"max_new_tokens\" not in kwargs:\n        kwargs[\"max_new_tokens\"] = 512\n\n    generated_ids = self.model.generate(**inputs, **kwargs)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    validation_output = self.processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n\n    # TODO: Need a better way to parse the output\n    try:\n        validation_output = validation_output.replace('```', '').replace('json', '')\n        validation_output = json.loads(validation_output)\n        validation_reasoning = validation_output[\"validation_reasoning\"]\n        confidence = validation_output[\"confidence\"]\n    except Exception as e:\n        logger.error(f\"Image caption validation parsing failed trying to parse using another logic.\")\n\n        number_str  = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in validation_output)\n        number_str = [i for i in number_str.split() if i.isalnum()]\n        potential_confidence_scores = [float(i) for i in number_str if float(i) &gt;= 0 and float(i) &lt;= 1]\n        confidence = max(potential_confidence_scores) if potential_confidence_scores else 0.0\n        validation_reasoning = validation_output\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#classification","title":"Classification","text":""},{"location":"image/#swiftannotate.image.classification.GeminiForImageClassification","title":"<code>GeminiForImageClassification</code>","text":"<p>               Bases: <code>BaseImageClassification</code></p> <p>GeminiForImageClassification pipeline for generating captions for images using Gemini models.</p> <p>Example usage: <pre><code>from swiftannotate.image import GeminiForImageClassification\n\n# Initialize the pipeline\nclassification_pipeline = GeminiForImageClassification(\n    caption_model=\"gemini-1.5-pro\",\n    validation_model=\"gemini-1.5-flash\",\n    api_key=\"your_api_key_here\",\n    classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = classification_pipeline.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         \"image_path\": 'path/to/image1.jpg', \n#         \"image_classification\": 'kitchen', \n#         \"validation_reasoning\": 'The class label is valid.', \n#         \"validation_score\": 0.6\n#     }, \n# ]\n</code></pre></p> Source code in <code>swiftannotate/image/classification/gemini.py</code> <pre><code>class GeminiForImageClassification(BaseImageClassification):\n    \"\"\"\n    GeminiForImageClassification pipeline for generating captions for images using Gemini models.\n\n    Example usage:\n    ```python\n    from swiftannotate.image import GeminiForImageClassification\n\n    # Initialize the pipeline\n    classification_pipeline = GeminiForImageClassification(\n        caption_model=\"gemini-1.5-pro\",\n        validation_model=\"gemini-1.5-flash\",\n        api_key=\"your_api_key_here\",\n        classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = classification_pipeline.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         \"image_path\": 'path/to/image1.jpg', \n    #         \"image_classification\": 'kitchen', \n    #         \"validation_reasoning\": 'The class label is valid.', \n    #         \"validation_score\": 0.6\n    #     }, \n    # ]\n    ```\n    \"\"\"\n\n    def __init__(\n        self, \n        classification_model: str, \n        validation_model: str,\n        api_key: str, \n        classification_labels: List[str],\n        classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n    ):\n        \"\"\"\n        Initializes the GeminiForImageClassification pipeline.\n\n        Args:\n            classification_model (str): \n                Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n            validation_model (str): \n                Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n            api_key (str): \n                Google Gemini API key.\n            classification_labels (List[str]):\n                List of classification labels to be used for the image classification.\n            classification_prompt (str | None, optional): \n                System prompt for classification images.\n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image class labels should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image class labels. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n\n            It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n            You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n        \"\"\"        \n        genai.configure(api_key=api_key)\n        self.classification_model = genai.GenerativeModel(model=classification_model)\n        self.validation_model = genai.GenerativeModel(model=validation_model)\n\n        super().__init__(\n            classification_labels=classification_labels,\n            classification_prompt=classification_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n        \"\"\"\n        Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the calss label does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated class label for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the class label you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Regenerate the class label for the given image.\n                Classify the given image as {', '.join(map(str, self.classification_labels))}\n            \"\"\"\n        else:\n            user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n        messages = [\n            self.classification_prompt,\n            {'mime_type':'image/jpeg', 'data': image}, \n            user_prompt\n        ]\n\n        try:\n            output = self.classification_model.generate_content(\n                messages,\n                generation_config=genai.GenerationConfig(\n                    response_mime_type=\"application/json\", \n                    response_schema=ImageClassificationOutputGemini,\n                    **kwargs\n                )\n            )\n            class_label = output[\"class_label\"].lower()\n        except Exception as e:\n            logger.error(f\"Image classification failed: {e}\")\n            class_label = \"ERROR\"\n\n        return class_label\n\n    def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]:\n        \"\"\"\n        Validates the class label generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            class_label (str): Class Label generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the class label.\n        \"\"\"\n        if class_label == \"ERROR\":\n            return \"ERROR\", 0.0\n\n        messages = [\n            self.validation_prompt,\n            {'mime_type':'image/jpeg', 'data': image},\n            class_label,\n            \"Validate the class label generated for the given image.\"\n        ]\n\n        try:\n            validation_output = self.validation_model.generate_content(\n                messages,\n                generation_config=genai.GenerationConfig(\n                    response_mime_type=\"application/json\", \n                    response_schema=ImageValidationOutputGemini\n                )\n            )\n            validation_reasoning = validation_output[\"validation_reasoning\"]\n            confidence = validation_output[\"confidence\"]\n        except Exception as e:\n            logger.error(f\"Image class label validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0.0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates class label for a list of images. \n\n        Args:\n            image_paths (List[str]): List of image paths to generate class labels for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n        \"\"\"\n\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results \n</code></pre>"},{"location":"image/#swiftannotate.image.classification.GeminiForImageClassification.__init__","title":"<code>__init__(classification_model, validation_model, api_key, classification_labels, classification_prompt=BASE_IMAGE_CLASSIFICATION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None)</code>","text":"<p>Initializes the GeminiForImageClassification pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>classification_model</code> <code>str</code> <p>Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.</p> required <code>validation_model</code> <code>str</code> <p>Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.</p> required <code>api_key</code> <code>str</code> <p>Google Gemini API key.</p> required <code>classification_labels</code> <code>List[str]</code> <p>List of classification labels to be used for the image classification.</p> required <code>classification_prompt</code> <code>str | None</code> <p>System prompt for classification images. Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image class labels should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image class labels is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image class labels.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> Notes <p><code>validation_prompt</code> should specify the rules for validating the class label and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> <p>It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels. You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.</p> Source code in <code>swiftannotate/image/classification/gemini.py</code> <pre><code>def __init__(\n    self, \n    classification_model: str, \n    validation_model: str,\n    api_key: str, \n    classification_labels: List[str],\n    classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n):\n    \"\"\"\n    Initializes the GeminiForImageClassification pipeline.\n\n    Args:\n        classification_model (str): \n            Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n        validation_model (str): \n            Can be either \"gemini-1.5-flash\", \"gemini-1.5-pro\", etc. or specific versions of model supported by Gemini.\n        api_key (str): \n            Google Gemini API key.\n        classification_labels (List[str]):\n            List of classification labels to be used for the image classification.\n        classification_prompt (str | None, optional): \n            System prompt for classification images.\n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image class labels should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image class labels. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n\n        It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n        You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n    \"\"\"        \n    genai.configure(api_key=api_key)\n    self.classification_model = genai.GenerativeModel(model=classification_model)\n    self.validation_model = genai.GenerativeModel(model=validation_model)\n\n    super().__init__(\n        classification_labels=classification_labels,\n        classification_prompt=classification_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.GeminiForImageClassification.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a class label. Implements the logic to generate class labels for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the calss label does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better class label. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated class label for the image.</p> Source code in <code>swiftannotate/image/classification/gemini.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n    \"\"\"\n    Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the calss label does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated class label for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the class label you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Regenerate the class label for the given image.\n            Classify the given image as {', '.join(map(str, self.classification_labels))}\n        \"\"\"\n    else:\n        user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n    messages = [\n        self.classification_prompt,\n        {'mime_type':'image/jpeg', 'data': image}, \n        user_prompt\n    ]\n\n    try:\n        output = self.classification_model.generate_content(\n            messages,\n            generation_config=genai.GenerationConfig(\n                response_mime_type=\"application/json\", \n                response_schema=ImageClassificationOutputGemini,\n                **kwargs\n            )\n        )\n        class_label = output[\"class_label\"].lower()\n    except Exception as e:\n        logger.error(f\"Image classification failed: {e}\")\n        class_label = \"ERROR\"\n\n    return class_label\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.GeminiForImageClassification.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates class label for a list of images. </p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate class labels for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of class labels, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/classification/gemini.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates class label for a list of images. \n\n    Args:\n        image_paths (List[str]): List of image paths to generate class labels for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n    \"\"\"\n\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results \n</code></pre>"},{"location":"image/#swiftannotate.image.classification.GeminiForImageClassification.validate","title":"<code>validate(image, class_label, **kwargs)</code>","text":"<p>Validates the class label generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>class_label</code> <code>str</code> <p>Class Label generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the class label.</p> Source code in <code>swiftannotate/image/classification/gemini.py</code> <pre><code>def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]:\n    \"\"\"\n    Validates the class label generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        class_label (str): Class Label generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the class label.\n    \"\"\"\n    if class_label == \"ERROR\":\n        return \"ERROR\", 0.0\n\n    messages = [\n        self.validation_prompt,\n        {'mime_type':'image/jpeg', 'data': image},\n        class_label,\n        \"Validate the class label generated for the given image.\"\n    ]\n\n    try:\n        validation_output = self.validation_model.generate_content(\n            messages,\n            generation_config=genai.GenerationConfig(\n                response_mime_type=\"application/json\", \n                response_schema=ImageValidationOutputGemini\n            )\n        )\n        validation_reasoning = validation_output[\"validation_reasoning\"]\n        confidence = validation_output[\"confidence\"]\n    except Exception as e:\n        logger.error(f\"Image class label validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0.0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OllamaForImageClassification","title":"<code>OllamaForImageClassification</code>","text":"<p>               Bases: <code>BaseImageClassification</code></p> <p>OllamaForImageClassification pipeline using OpenAI API.</p> <p>Example usage:</p> <pre><code>from swiftannotate.image import OllamaForImageClassification\n\n# Initialize the pipeline\nclassification_pipeline = OllamaForImageClassification(\n    classification_model=\"llama3.2-vision\",\n    validation_model=\"llama3.2-vision\",\n    classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = classification_pipeline.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         \"image_path\": 'path/to/image1.jpg', \n#         \"image_classification\": 'kitchen', \n#         \"validation_reasoning\": 'The class label is valid.', \n#         \"validation_score\": 0.6\n#     }, \n# ]\n</code></pre> Source code in <code>swiftannotate/image/classification/ollama.py</code> <pre><code>class OllamaForImageClassification(BaseImageClassification):\n    \"\"\"\n    OllamaForImageClassification pipeline using OpenAI API.\n\n    Example usage:\n\n    ```python\n    from swiftannotate.image import OllamaForImageClassification\n\n    # Initialize the pipeline\n    classification_pipeline = OllamaForImageClassification(\n        classification_model=\"llama3.2-vision\",\n        validation_model=\"llama3.2-vision\",\n        classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = classification_pipeline.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         \"image_path\": 'path/to/image1.jpg', \n    #         \"image_classification\": 'kitchen', \n    #         \"validation_reasoning\": 'The class label is valid.', \n    #         \"validation_score\": 0.6\n    #     }, \n    # ]\n    ```\n    \"\"\"\n    def __init__(\n        self, \n        classification_model: str, \n        validation_model: str,\n        classification_labels: List[str],\n        classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n    ):\n        \"\"\"\n        Initializes the OllamaForImageClassification pipeline.\n\n        Args:\n            classification_model (str): \n                Can be either any of the Multimodal (Vision) models supported by Ollama.\n                specific versions of model supported by Ollama.\n            validation_model (str): \n                Can be either any of the Multimodal (Vision) models supported by Ollama.\n                specific versions of model supported by Ollama.\n            classification_labels (List[str]):\n                List of classification labels to be used for the image classification.\n            classification_prompt (str | None, optional): \n                System prompt for classification images.\n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image class labels should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image class labels. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n\n            It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n            You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n        \"\"\"\n\n        if not self._validate_ollama_model(classification_model):\n            raise ValueError(f\"Model {classification_model} is not supported by Ollama.\")\n\n        if not self._validate_ollama_model(validation_model):\n            raise ValueError(f\"Model {validation_model} is not supported by Ollama.\")\n\n        self.classification_model = classification_model\n        self.validation_model = validation_model\n\n        super().__init__(\n            classification_labels=classification_labels,\n            classification_prompt=classification_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n    def _validate_ollama_model(self, model: str) -&gt; bool:\n        try:\n            ollama.chat(model)\n        except ollama.ResponseError as e:\n            logger.error(f\"Error: {e.error}\")\n            if e.status_code == 404:\n                try:\n                    ollama.pull(model)\n                    logger.info(f\"Model {model} is now downloaded.\")\n                except ollama.ResponseError as e:\n                    logger.error(f\"Error: {e.error}\")\n                    logger.error(f\"Model {model} could not be downloaded. Check the model name and try again.\")\n                    return False\n            logger.info(f\"Model {model} is now downloaded.\")\n\n        return True\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n        \"\"\"\n        Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the calss label does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated class label for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the class label you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Regenerate the class label for the given image.\n                Classify the given image as {', '.join(map(str, self.classification_labels))}\n            \"\"\"\n        else:\n            user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n        messages=[\n            {\"role\": \"system\", \"content\": self.classification_prompt},\n            {\n                \"role\": \"user\",\n                \"images\": [image],\n                \"content\": user_prompt,\n            }\n        ]\n\n        if not \"temperature\" in kwargs:\n            kwargs[\"temperature\"] = 0.0\n\n        try:  \n            response = ollama.chat(\n                model=self.classification_model,\n                messages=messages,\n                format=ImageClassificationOutputOllama.model_json_schema(),\n                options=kwargs\n            )\n\n            output = ImageClassificationOutputOllama.model_validate_json(response.message.content)\n            class_label = output.class_label.lower()\n\n        except Exception as e:\n            logger.error(f\"Image classification failed: {e}\")\n            class_label = \"ERROR\"\n\n        return class_label\n\n    def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]: \n        \"\"\"\n        Validates the class label generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            class_label (str): Class Label generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the class label.\n        \"\"\"\n        if class_label == \"ERROR\":\n            return \"ERROR\", 0\n\n        messages = [\n            {\n                \"role\": \"system\", \"content\": self.validation_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"images\": [image],\n                \"content\": class_label + \"\\nValidate the class label generated for the given image.\"\n            }\n        ] \n\n        if not \"temperature\" in kwargs:\n            kwargs[\"temperature\"] = 0.0     \n\n        try:\n            response = ollama.chat(\n                model=self.validation_model,\n                messages=messages,\n                format=ImageValidationOutputOllama.model_json_schema(),\n                options=kwargs\n            )\n\n            validation_output = ImageValidationOutputOllama.model_validate_json(response.message.content)\n\n            validation_reasoning = validation_output.validation_reasoning\n            confidence = validation_output.confidence\n\n        except Exception as e:\n            logger.error(f\"Image class label validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates class label for a list of images. \n\n        Args:\n            image_paths (List[str]): List of image paths to generate class labels for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OllamaForImageClassification.__init__","title":"<code>__init__(classification_model, validation_model, classification_labels, classification_prompt=BASE_IMAGE_CLASSIFICATION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None)</code>","text":"<p>Initializes the OllamaForImageClassification pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>classification_model</code> <code>str</code> <p>Can be either any of the Multimodal (Vision) models supported by Ollama. specific versions of model supported by Ollama.</p> required <code>validation_model</code> <code>str</code> <p>Can be either any of the Multimodal (Vision) models supported by Ollama. specific versions of model supported by Ollama.</p> required <code>classification_labels</code> <code>List[str]</code> <p>List of classification labels to be used for the image classification.</p> required <code>classification_prompt</code> <code>str | None</code> <p>System prompt for classification images. Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image class labels should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image class labels is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image class labels.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> Notes <p><code>validation_prompt</code> should specify the rules for validating the class label and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> <p>It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels. You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.</p> Source code in <code>swiftannotate/image/classification/ollama.py</code> <pre><code>def __init__(\n    self, \n    classification_model: str, \n    validation_model: str,\n    classification_labels: List[str],\n    classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n):\n    \"\"\"\n    Initializes the OllamaForImageClassification pipeline.\n\n    Args:\n        classification_model (str): \n            Can be either any of the Multimodal (Vision) models supported by Ollama.\n            specific versions of model supported by Ollama.\n        validation_model (str): \n            Can be either any of the Multimodal (Vision) models supported by Ollama.\n            specific versions of model supported by Ollama.\n        classification_labels (List[str]):\n            List of classification labels to be used for the image classification.\n        classification_prompt (str | None, optional): \n            System prompt for classification images.\n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image class labels should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image class labels. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n\n        It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n        You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n    \"\"\"\n\n    if not self._validate_ollama_model(classification_model):\n        raise ValueError(f\"Model {classification_model} is not supported by Ollama.\")\n\n    if not self._validate_ollama_model(validation_model):\n        raise ValueError(f\"Model {validation_model} is not supported by Ollama.\")\n\n    self.classification_model = classification_model\n    self.validation_model = validation_model\n\n    super().__init__(\n        classification_labels=classification_labels,\n        classification_prompt=classification_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OllamaForImageClassification.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a class label. Implements the logic to generate class labels for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the calss label does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better class label. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated class label for the image.</p> Source code in <code>swiftannotate/image/classification/ollama.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n    \"\"\"\n    Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the calss label does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated class label for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the class label you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Regenerate the class label for the given image.\n            Classify the given image as {', '.join(map(str, self.classification_labels))}\n        \"\"\"\n    else:\n        user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n    messages=[\n        {\"role\": \"system\", \"content\": self.classification_prompt},\n        {\n            \"role\": \"user\",\n            \"images\": [image],\n            \"content\": user_prompt,\n        }\n    ]\n\n    if not \"temperature\" in kwargs:\n        kwargs[\"temperature\"] = 0.0\n\n    try:  \n        response = ollama.chat(\n            model=self.classification_model,\n            messages=messages,\n            format=ImageClassificationOutputOllama.model_json_schema(),\n            options=kwargs\n        )\n\n        output = ImageClassificationOutputOllama.model_validate_json(response.message.content)\n        class_label = output.class_label.lower()\n\n    except Exception as e:\n        logger.error(f\"Image classification failed: {e}\")\n        class_label = \"ERROR\"\n\n    return class_label\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OllamaForImageClassification.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates class label for a list of images. </p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate class labels for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of class labels, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/classification/ollama.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates class label for a list of images. \n\n    Args:\n        image_paths (List[str]): List of image paths to generate class labels for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OllamaForImageClassification.validate","title":"<code>validate(image, class_label, **kwargs)</code>","text":"<p>Validates the class label generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>class_label</code> <code>str</code> <p>Class Label generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the class label.</p> Source code in <code>swiftannotate/image/classification/ollama.py</code> <pre><code>def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]: \n    \"\"\"\n    Validates the class label generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        class_label (str): Class Label generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the class label.\n    \"\"\"\n    if class_label == \"ERROR\":\n        return \"ERROR\", 0\n\n    messages = [\n        {\n            \"role\": \"system\", \"content\": self.validation_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"images\": [image],\n            \"content\": class_label + \"\\nValidate the class label generated for the given image.\"\n        }\n    ] \n\n    if not \"temperature\" in kwargs:\n        kwargs[\"temperature\"] = 0.0     \n\n    try:\n        response = ollama.chat(\n            model=self.validation_model,\n            messages=messages,\n            format=ImageValidationOutputOllama.model_json_schema(),\n            options=kwargs\n        )\n\n        validation_output = ImageValidationOutputOllama.model_validate_json(response.message.content)\n\n        validation_reasoning = validation_output.validation_reasoning\n        confidence = validation_output.confidence\n\n    except Exception as e:\n        logger.error(f\"Image class label validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OpenAIForImageClassification","title":"<code>OpenAIForImageClassification</code>","text":"<p>               Bases: <code>BaseImageClassification</code></p> <p>OpenAIForImageClassification pipeline using OpenAI API.</p> <p>Example usage:</p> <pre><code>from swiftannotate.image import OpenAIForImageClassification\n\n# Initialize the pipeline\nclassification_pipeline = OpenAIForImageClassification(\n    classification_model=\"gpt-4o\",\n    validation_model=\"gpt-4o-mini\",\n    api_key=\"your_api_key_here\",\n    classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n    output_file=\"captions.json\"\n)\n\n# Generate captions for a list of images\nimage_paths = [\"path/to/image1.jpg\"]\nresults = classification_pipeline.generate(image_paths)\n\n# Print results\n# Output: [\n#     {\n#         \"image_path\": 'path/to/image1.jpg', \n#         \"image_classification\": 'kitchen', \n#         \"validation_reasoning\": 'The class label is valid.', \n#         \"validation_score\": 0.6\n#     }, \n# ]\n</code></pre> Source code in <code>swiftannotate/image/classification/openai.py</code> <pre><code>class OpenAIForImageClassification(BaseImageClassification):\n    \"\"\"\n    OpenAIForImageClassification pipeline using OpenAI API.\n\n    Example usage:\n\n    ```python\n    from swiftannotate.image import OpenAIForImageClassification\n\n    # Initialize the pipeline\n    classification_pipeline = OpenAIForImageClassification(\n        classification_model=\"gpt-4o\",\n        validation_model=\"gpt-4o-mini\",\n        api_key=\"your_api_key_here\",\n        classification_labels=[\"kitchen\", \"bedroom\", \"living room\"],\n        output_file=\"captions.json\"\n    )\n\n    # Generate captions for a list of images\n    image_paths = [\"path/to/image1.jpg\"]\n    results = classification_pipeline.generate(image_paths)\n\n    # Print results\n    # Output: [\n    #     {\n    #         \"image_path\": 'path/to/image1.jpg', \n    #         \"image_classification\": 'kitchen', \n    #         \"validation_reasoning\": 'The class label is valid.', \n    #         \"validation_score\": 0.6\n    #     }, \n    # ]\n    ```\n    \"\"\"\n    def __init__(\n        self, \n        classification_model: str, \n        validation_model: str,\n        api_key: str, \n        classification_labels: List[str],\n        classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n        **kwargs\n    ):\n        \"\"\"\n        Initializes the OpenAIForImageClassification pipeline.\n\n        Args:\n            classification_model (str): \n                Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n                specific versions of model supported by OpenAI.\n            validation_model (str): \n                Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n                specific versions of model supported by OpenAI.\n            api_key (str): OpenAI API key.\n            classification_labels (List[str]):\n                List of classification labels to be used for the image classification.\n            classification_prompt (str | None, optional): \n                System prompt for classification images.\n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image class labels should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image class labels. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Keyword Arguments:\n            detail (str, optional): \n                Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n\n            It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n            You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n        \"\"\"\n        self.classification_model = classification_model\n        self.validation_model = validation_model\n        self.client = OpenAI(api_key)\n\n        super().__init__(\n            classification_labels=classification_labels,\n            classification_prompt=classification_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n        self.detail = kwargs.get(\"detail\", \"low\")\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n        \"\"\"\n        Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the calss label does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated class label for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the class label you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Regenerate the class label for the given image.\n                Classify the given image as {', '.join(map(str, self.classification_labels))}\n            \"\"\"\n        else:\n            user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n        messages=[\n            {\"role\": \"system\", \"content\": self.classification_prompt},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ]\n            }\n        ]\n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.classification_model,\n                messages=messages,\n                response_format=ImageClassificationOutputOpenAI,\n                **kwargs\n            )\n            output = response.choices[0].message.parsed\n            class_label = output.class_label.lower()\n        except Exception as e:\n            logger.error(f\"Image classification failed: {e}\")\n            class_label = \"ERROR\"\n\n        return class_label\n\n    def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]: \n        \"\"\"\n        Validates the class label generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            class_label (str): Class Label generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the class label.\n        \"\"\"\n        if class_label == \"ERROR\":\n            return \"ERROR\", 0\n\n        messages = [\n            {\n                \"role\": \"system\",\n                \"content\": self.validation_prompt\n            },\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\n                        \"type\": \"image_url\",\n                        \"image_url\": {\n                            \"url\": f\"data:image/jpeg;base64,{image}\",\n                            \"detail\": self.detail\n                        },\n                    },\n                    {\n                        \"type\": \"text\",\n                        \"text\": class_label + \"\\nValidate the class label generated for the given image.\"\n                    }\n                ]\n            }\n        ]      \n\n        try:\n            response = self.client.chat.completions.create(\n                model=self.validation_model,\n                messages=messages,\n                response_format=ImageValidationOutputOpenAI,\n                **kwargs\n            )\n            validation_output = response.choices[0].message.parsed\n            validation_reasoning = validation_output.validation_reasoning\n            confidence = validation_output.confidence\n\n        except Exception as e:\n            logger.error(f\"Image class label validation failed: {e}\")\n            validation_reasoning = \"ERROR\"\n            confidence = 0\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates class label for a list of images. \n\n        Args:\n            image_paths (List[str]): List of image paths to generate class labels for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OpenAIForImageClassification.__init__","title":"<code>__init__(classification_model, validation_model, api_key, classification_labels, classification_prompt=BASE_IMAGE_CLASSIFICATION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None, **kwargs)</code>","text":"<p>Initializes the OpenAIForImageClassification pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>classification_model</code> <code>str</code> <p>Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or  specific versions of model supported by OpenAI.</p> required <code>validation_model</code> <code>str</code> <p>Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or  specific versions of model supported by OpenAI.</p> required <code>api_key</code> <code>str</code> <p>OpenAI API key.</p> required <code>classification_labels</code> <code>List[str]</code> <p>List of classification labels to be used for the image classification.</p> required <code>classification_prompt</code> <code>str | None</code> <p>System prompt for classification images. Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image class labels should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image class labels is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image class labels.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>detail</code> <code>str</code> <p>Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".</p> Notes <p><code>validation_prompt</code> should specify the rules for validating the class label and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> <p>It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels. You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.</p> Source code in <code>swiftannotate/image/classification/openai.py</code> <pre><code>def __init__(\n    self, \n    classification_model: str, \n    validation_model: str,\n    api_key: str, \n    classification_labels: List[str],\n    classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n    **kwargs\n):\n    \"\"\"\n    Initializes the OpenAIForImageClassification pipeline.\n\n    Args:\n        classification_model (str): \n            Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n            specific versions of model supported by OpenAI.\n        validation_model (str): \n            Can be either \"gpt-4o\", \"gpt-4o-mini\", etc. or \n            specific versions of model supported by OpenAI.\n        api_key (str): OpenAI API key.\n        classification_labels (List[str]):\n            List of classification labels to be used for the image classification.\n        classification_prompt (str | None, optional): \n            System prompt for classification images.\n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image class labels should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image class labels. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Keyword Arguments:\n        detail (str, optional): \n            Specific to OpenAI. Detail level of the image (Higher resolution costs more). Defaults to \"low\".\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n\n        It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n        You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n    \"\"\"\n    self.classification_model = classification_model\n    self.validation_model = validation_model\n    self.client = OpenAI(api_key)\n\n    super().__init__(\n        classification_labels=classification_labels,\n        classification_prompt=classification_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n\n    self.detail = kwargs.get(\"detail\", \"low\")\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OpenAIForImageClassification.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a class label. Implements the logic to generate class labels for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the calss label does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better class label. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated class label for the image.</p> Source code in <code>swiftannotate/image/classification/openai.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:        \n    \"\"\"\n    Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the calss label does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated class label for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the class label you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Regenerate the class label for the given image.\n            Classify the given image as {', '.join(map(str, self.classification_labels))}\n        \"\"\"\n    else:\n        user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))}\"\n\n    messages=[\n        {\"role\": \"system\", \"content\": self.classification_prompt},\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\n                        \"detail\": self.detail\n                    },\n                },\n                {\"type\": \"text\", \"text\": user_prompt},\n            ]\n        }\n    ]\n\n    try:\n        response = self.client.chat.completions.create(\n            model=self.classification_model,\n            messages=messages,\n            response_format=ImageClassificationOutputOpenAI,\n            **kwargs\n        )\n        output = response.choices[0].message.parsed\n        class_label = output.class_label.lower()\n    except Exception as e:\n        logger.error(f\"Image classification failed: {e}\")\n        class_label = \"ERROR\"\n\n    return class_label\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OpenAIForImageClassification.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates class label for a list of images. </p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate class labels for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of class labels, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/classification/openai.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates class label for a list of images. \n\n    Args:\n        image_paths (List[str]): List of image paths to generate class labels for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.OpenAIForImageClassification.validate","title":"<code>validate(image, class_label, **kwargs)</code>","text":"<p>Validates the class label generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>class_label</code> <code>str</code> <p>Class Label generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the class label.</p> Source code in <code>swiftannotate/image/classification/openai.py</code> <pre><code>def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]: \n    \"\"\"\n    Validates the class label generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        class_label (str): Class Label generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the class label.\n    \"\"\"\n    if class_label == \"ERROR\":\n        return \"ERROR\", 0\n\n    messages = [\n        {\n            \"role\": \"system\",\n            \"content\": self.validation_prompt\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\n                        \"detail\": self.detail\n                    },\n                },\n                {\n                    \"type\": \"text\",\n                    \"text\": class_label + \"\\nValidate the class label generated for the given image.\"\n                }\n            ]\n        }\n    ]      \n\n    try:\n        response = self.client.chat.completions.create(\n            model=self.validation_model,\n            messages=messages,\n            response_format=ImageValidationOutputOpenAI,\n            **kwargs\n        )\n        validation_output = response.choices[0].message.parsed\n        validation_reasoning = validation_output.validation_reasoning\n        confidence = validation_output.confidence\n\n    except Exception as e:\n        logger.error(f\"Image class label validation failed: {e}\")\n        validation_reasoning = \"ERROR\"\n        confidence = 0\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.Qwen2VLForImageClassification","title":"<code>Qwen2VLForImageClassification</code>","text":"<p>               Bases: <code>BaseImageClassification</code></p> <p>Qwen2VLForImageClassification pipeline using Qwen2VL model.</p> <p>Example usage: <pre><code>from transformers import AutoProcessor, AutoModelForImageTextToText\nfrom transformers import BitsAndBytesConfig\nfrom swiftannotate.image import Qwen2VLForImageClassification\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=\"float16\",\n    bnb_4bit_use_double_quant=True\n)\n\nmodel = AutoModelForImageTextToText.from_pretrained(\n    \"Qwen/Qwen2-VL-7B-Instruct\",\n    device_map=\"auto\",\n    torch_dtype=\"auto\",\n    quantization_config=quantization_config)\n\nprocessor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n# Load the Caption Model\nkwargs = {\"temperature\": 0}\nclassification_pipeline = Qwen2VLForImageClassification(\n    model=model,\n    processor=processor,\n    classification_labels=[\"kitchen\", \"bottle\", \"none\"],\n    output_file=\"output.json\",\n)\n\n# Generate captions for images\nimage_paths = ['path/to/image1.jpg']\nresults = classification_pipeline.generate(image_paths, **kwargs)\n\n# Print results\n# Output: [\n#     {\n#         \"image_path\": 'path/to/image1.jpg', \n#         \"image_classification\": 'kitchen', \n#         \"validation_reasoning\": 'The class label is valid.', \n#         \"validation_score\": 0.6\n#     }, \n# ]\n</code></pre></p> Source code in <code>swiftannotate/image/classification/qwen.py</code> <pre><code>class Qwen2VLForImageClassification(BaseImageClassification):\n    \"\"\"\n    Qwen2VLForImageClassification pipeline using Qwen2VL model.\n\n    Example usage:\n    ```python\n    from transformers import AutoProcessor, AutoModelForImageTextToText\n    from transformers import BitsAndBytesConfig\n    from swiftannotate.image import Qwen2VLForImageClassification\n\n    quantization_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=\"float16\",\n        bnb_4bit_use_double_quant=True\n    )\n\n    model = AutoModelForImageTextToText.from_pretrained(\n        \"Qwen/Qwen2-VL-7B-Instruct\",\n        device_map=\"auto\",\n        torch_dtype=\"auto\",\n        quantization_config=quantization_config)\n\n    processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n\n    # Load the Caption Model\n    kwargs = {\"temperature\": 0}\n    classification_pipeline = Qwen2VLForImageClassification(\n        model=model,\n        processor=processor,\n        classification_labels=[\"kitchen\", \"bottle\", \"none\"],\n        output_file=\"output.json\",\n    )\n\n    # Generate captions for images\n    image_paths = ['path/to/image1.jpg']\n    results = classification_pipeline.generate(image_paths, **kwargs)\n\n    # Print results\n    # Output: [\n    #     {\n    #         \"image_path\": 'path/to/image1.jpg', \n    #         \"image_classification\": 'kitchen', \n    #         \"validation_reasoning\": 'The class label is valid.', \n    #         \"validation_score\": 0.6\n    #     }, \n    # ]\n    ```\n    \"\"\"\n    def __init__(\n        self, \n        model: AutoModelForImageTextToText | Qwen2VLForConditionalGeneration, \n        processor: AutoProcessor | Qwen2VLProcessor,\n        classification_labels: List[str],\n        classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n        validation: bool = True,\n        validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n        validation_threshold: float = 0.5,\n        max_retry: int = 3, \n        output_file: str | None = None,\n        **kwargs\n    ):     \n        \"\"\"\n        Initializes the Qwen2VLForImageClassification pipeline.\n\n        Args:\n            model (AutoModelForImageTextToText): \n                Model for image classification. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights.\n                Can be any version of Qwen2-VL model (7B, 72B).\n            processor (AutoProcessor): \n                Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.\n            classification_labels (List[str]):\n                List of classification labels to be used for the image classification.\n            classification_prompt (str | None, optional): \n                System prompt for classification images.\n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation (bool, optional): \n                Use validation step or not. Defaults to True.\n            validation_prompt (str | None, optional): \n                System prompt for validating image class labels should specify the range of validation score to be generated. \n                Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n            validation_threshold (float, optional): \n                Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n                Defaults to 0.5.\n            max_retry (int, optional):\n                Number of retries before giving up on the image class labels. \n                Defaults to 3.\n            output_file (str | None, optional): \n                Output file path, only JSON is supported for now. \n                Defaults to None.\n\n        Keyword Arguments:\n            resize_height (int, optional):\n                Height to resize the image before generating class labels. Defaults to 280.\n            resize_width (int, optional):\n                Width to resize the image before generating class labels. Defaults to 420.\n\n        Notes:\n            `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n            Your `validation_threshold` should be within this specified range.\n\n            It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n            You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n        \"\"\"    \n\n        if not isinstance(model, Qwen2VLForConditionalGeneration):\n            raise ValueError(\"Model should be an instance of Qwen2VLForConditionalGeneration.\")\n        if not isinstance(processor, Qwen2VLProcessor):\n            raise ValueError(\"Processor should be an instance of Qwen2VLProcessor.\")\n\n        self.model = model\n        self.processor = processor\n\n        super().__init__(\n            classification_labels=classification_labels,\n            classification_prompt=classification_prompt,\n            validation=validation,\n            validation_prompt=validation_prompt,\n            validation_threshold=validation_threshold,\n            max_retry=max_retry,\n            output_file=output_file\n        )\n\n        self.resize_height = kwargs.get(\"resize_height\", 280)\n        self.resize_width = kwargs.get(\"resize_width\", 420)\n\n    def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n        \"\"\"\n        Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n        **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n        the previous iteration in case the calss label does not pass validation threshold.\n\n        Args:\n            image (str): Base64 encoded image.\n            feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            str: Generated class label for the image.\n        \"\"\"\n        if feedback_prompt:\n            user_prompt = f\"\"\"\n                Last time the class label you generated for this image was incorrect because of the following reasons:\n                {feedback_prompt}\n\n                Regenerate the class label for the given image.\n                Classify the given image as {', '.join(map(str, self.classification_labels))}\n                Return output as a JSON object with key as 'class_label'\n            \"\"\"\n        else:\n            user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))} \\nReturn output as a JSON object with key as 'class_label'\"\n\n        messages = [\n            {\"role\": \"system\", \"content\": self.classification_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": user_prompt},\n                ],\n            },\n        ]\n\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(self.model.device)\n\n        # Inference: Generation of the output\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 512\n\n        generated_ids = self.model.generate(**inputs, **kwargs)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        class_label = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        try:\n            class_label = class_label.replace('```', '').replace('json', '')\n            class_label = json.loads(class_label)\n            class_label = class_label[\"class_label\"].lower()\n        except Exception as e:\n            logger.error(f\"Image classification parsing failed trying to parse using another logic.\")\n            potential_class_labels = [label.lower() for label in class_label.split() if label in self.classification_labels]\n            class_label = potential_class_labels[0] if potential_class_labels else \"ERROR\"\n\n        return class_label\n\n    def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]:\n        \"\"\"\n        Validates the class label generated for the image.\n\n        Args:\n            image (str): Base64 encoded image.\n            class_label (str): Class Label generated for the image.\n\n        Returns:\n            Tuple[str, float]: Validation reasoning and confidence score for the class label.\n        \"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": self.validation_prompt},\n            {\n                \"role\": \"user\", \n                \"content\": [\n                    {\n                        \"type\": \"image\", \n                        \"image\": f\"data:image;base64,{image}\",\n                        \"resized_height\": self.resize_height,\n                        \"resized_width\": self.resize_width,\n                    },\n                    {\"type\": \"text\", \"text\": class_label},\n                    {\n                        \"type\": \"text\", \n                        \"text\": \"\"\"\n                        Validate the class label generated for the given image. \n                        Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                        \"\"\"\n                    },\n                ],\n            },\n        ]\n\n        text = self.processor.apply_chat_template(\n            messages, tokenize=False, add_generation_prompt=True\n        )\n\n        image_inputs, video_inputs = process_vision_info(messages)\n        inputs = self.processor(\n            text=[text],\n            images=image_inputs,\n            videos=video_inputs,\n            padding=True,\n            return_tensors=\"pt\",\n        )\n        inputs = inputs.to(self.model.device)\n\n        # Inference: Generation of the output\n        if \"max_new_tokens\" not in kwargs:\n            kwargs[\"max_new_tokens\"] = 512\n\n        generated_ids = self.model.generate(**inputs, **kwargs)\n        generated_ids_trimmed = [\n            out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n        ]\n        validation_output = self.processor.batch_decode(\n            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n        )[0]\n\n        # TODO: Need a better way to parse the output\n        try:\n            validation_output = validation_output.replace('```', '').replace('json', '')\n            validation_output = json.loads(validation_output)\n            validation_reasoning = validation_output[\"validation_reasoning\"]\n            confidence = validation_output[\"confidence\"]\n        except Exception as e:\n            logger.error(f\"Image class label validation parsing failed trying to parse using another logic.\")\n\n            number_str  = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in validation_output)\n            number_str = [i for i in number_str.split() if i.isalnum()]\n            potential_confidence_scores = [float(i) for i in number_str if float(i) &gt;= 0 and float(i) &lt;= 1]\n            confidence = max(potential_confidence_scores) if potential_confidence_scores else 0.0\n            validation_reasoning = validation_output\n\n        return validation_reasoning, confidence\n\n    def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n        \"\"\"\n        Generates class label for a list of images. \n\n        Args:\n            image_paths (List[str]): List of image paths to generate class labels for.\n            **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n        Returns:\n            List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n        \"\"\"\n        results = super().generate(\n            image_paths=image_paths, \n            **kwargs\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.Qwen2VLForImageClassification.__init__","title":"<code>__init__(model, processor, classification_labels, classification_prompt=BASE_IMAGE_CLASSIFICATION_PROMPT, validation=True, validation_prompt=BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT, validation_threshold=0.5, max_retry=3, output_file=None, **kwargs)</code>","text":"<p>Initializes the Qwen2VLForImageClassification pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>AutoModelForImageTextToText</code> <p>Model for image classification. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights. Can be any version of Qwen2-VL model (7B, 72B).</p> required <code>processor</code> <code>AutoProcessor</code> <p>Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.</p> required <code>classification_labels</code> <code>List[str]</code> <p>List of classification labels to be used for the image classification.</p> required <code>classification_prompt</code> <code>str | None</code> <p>System prompt for classification images. Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_PROMPT</code> <code>validation</code> <code>bool</code> <p>Use validation step or not. Defaults to True.</p> <code>True</code> <code>validation_prompt</code> <code>str | None</code> <p>System prompt for validating image class labels should specify the range of validation score to be generated.  Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.</p> <code>BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT</code> <code>validation_threshold</code> <code>float</code> <p>Threshold to determine if image class labels is valid or not should be within specified range for validation score.  Defaults to 0.5.</p> <code>0.5</code> <code>max_retry</code> <code>int</code> <p>Number of retries before giving up on the image class labels.  Defaults to 3.</p> <code>3</code> <code>output_file</code> <code>str | None</code> <p>Output file path, only JSON is supported for now.  Defaults to None.</p> <code>None</code> <p>Other Parameters:</p> Name Type Description <code>resize_height</code> <code>int</code> <p>Height to resize the image before generating class labels. Defaults to 280.</p> <code>resize_width</code> <code>int</code> <p>Width to resize the image before generating class labels. Defaults to 420.</p> Notes <p><code>validation_prompt</code> should specify the rules for validating the class label and the range of validation score to be generated example (0-1). Your <code>validation_threshold</code> should be within this specified range.</p> <p>It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels. You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.</p> Source code in <code>swiftannotate/image/classification/qwen.py</code> <pre><code>def __init__(\n    self, \n    model: AutoModelForImageTextToText | Qwen2VLForConditionalGeneration, \n    processor: AutoProcessor | Qwen2VLProcessor,\n    classification_labels: List[str],\n    classification_prompt: str = BASE_IMAGE_CLASSIFICATION_PROMPT, \n    validation: bool = True,\n    validation_prompt: str = BASE_IMAGE_CLASSIFICATION_VALIDATION_PROMPT,\n    validation_threshold: float = 0.5,\n    max_retry: int = 3, \n    output_file: str | None = None,\n    **kwargs\n):     \n    \"\"\"\n    Initializes the Qwen2VLForImageClassification pipeline.\n\n    Args:\n        model (AutoModelForImageTextToText): \n            Model for image classification. Should be an instance of AutoModelForImageTextToText with Qwen2-VL pretrained weights.\n            Can be any version of Qwen2-VL model (7B, 72B).\n        processor (AutoProcessor): \n            Processor for the Qwen2-VL model. Should be an instance of AutoProcessor.\n        classification_labels (List[str]):\n            List of classification labels to be used for the image classification.\n        classification_prompt (str | None, optional): \n            System prompt for classification images.\n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation (bool, optional): \n            Use validation step or not. Defaults to True.\n        validation_prompt (str | None, optional): \n            System prompt for validating image class labels should specify the range of validation score to be generated. \n            Uses default BASE_IMAGE_CLASSIFICATION_PROMPT prompt if not provided.\n        validation_threshold (float, optional): \n            Threshold to determine if image class labels is valid or not should be within specified range for validation score. \n            Defaults to 0.5.\n        max_retry (int, optional):\n            Number of retries before giving up on the image class labels. \n            Defaults to 3.\n        output_file (str | None, optional): \n            Output file path, only JSON is supported for now. \n            Defaults to None.\n\n    Keyword Arguments:\n        resize_height (int, optional):\n            Height to resize the image before generating class labels. Defaults to 280.\n        resize_width (int, optional):\n            Width to resize the image before generating class labels. Defaults to 420.\n\n    Notes:\n        `validation_prompt` should specify the rules for validating the class label and the range of validation score to be generated example (0-1).\n        Your `validation_threshold` should be within this specified range.\n\n        It is advised to include class descriptions in the classification_prompt and validation_prompt to help the model understand the context of the class labels.\n        You can also add Few-shot learning examples to the prompt to help the model understand the context of the class labels.\n    \"\"\"    \n\n    if not isinstance(model, Qwen2VLForConditionalGeneration):\n        raise ValueError(\"Model should be an instance of Qwen2VLForConditionalGeneration.\")\n    if not isinstance(processor, Qwen2VLProcessor):\n        raise ValueError(\"Processor should be an instance of Qwen2VLProcessor.\")\n\n    self.model = model\n    self.processor = processor\n\n    super().__init__(\n        classification_labels=classification_labels,\n        classification_prompt=classification_prompt,\n        validation=validation,\n        validation_prompt=validation_prompt,\n        validation_threshold=validation_threshold,\n        max_retry=max_retry,\n        output_file=output_file\n    )\n\n    self.resize_height = kwargs.get(\"resize_height\", 280)\n    self.resize_width = kwargs.get(\"resize_width\", 420)\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.Qwen2VLForImageClassification.annotate","title":"<code>annotate(image, feedback_prompt='', **kwargs)</code>","text":"<p>Annotates the image with a class label. Implements the logic to generate class labels for an image.</p> <p>Note: The feedback_prompt is dynamically updated using the validation reasoning from  the previous iteration in case the calss label does not pass validation threshold.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>feedback_prompt</code> <code>str</code> <p>Feedback prompt for the user to generate a better class label. Defaults to ''.</p> <code>''</code> <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Generated class label for the image.</p> Source code in <code>swiftannotate/image/classification/qwen.py</code> <pre><code>def annotate(self, image: str, feedback_prompt:str = \"\", **kwargs) -&gt; str:\n    \"\"\"\n    Annotates the image with a class label. Implements the logic to generate class labels for an image.\n\n    **Note**: The feedback_prompt is dynamically updated using the validation reasoning from \n    the previous iteration in case the calss label does not pass validation threshold.\n\n    Args:\n        image (str): Base64 encoded image.\n        feedback_prompt (str, optional): Feedback prompt for the user to generate a better class label. Defaults to ''.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        str: Generated class label for the image.\n    \"\"\"\n    if feedback_prompt:\n        user_prompt = f\"\"\"\n            Last time the class label you generated for this image was incorrect because of the following reasons:\n            {feedback_prompt}\n\n            Regenerate the class label for the given image.\n            Classify the given image as {', '.join(map(str, self.classification_labels))}\n            Return output as a JSON object with key as 'class_label'\n        \"\"\"\n    else:\n        user_prompt = f\"Classify the given image as {', '.join(map(str, self.classification_labels))} \\nReturn output as a JSON object with key as 'class_label'\"\n\n    messages = [\n        {\"role\": \"system\", \"content\": self.classification_prompt},\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"data:image;base64,{image}\",\n                    \"resized_height\": self.resize_height,\n                    \"resized_width\": self.resize_width,\n                },\n                {\"type\": \"text\", \"text\": user_prompt},\n            ],\n        },\n    ]\n\n    text = self.processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = self.processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(self.model.device)\n\n    # Inference: Generation of the output\n    if \"max_new_tokens\" not in kwargs:\n        kwargs[\"max_new_tokens\"] = 512\n\n    generated_ids = self.model.generate(**inputs, **kwargs)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    class_label = self.processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n\n    try:\n        class_label = class_label.replace('```', '').replace('json', '')\n        class_label = json.loads(class_label)\n        class_label = class_label[\"class_label\"].lower()\n    except Exception as e:\n        logger.error(f\"Image classification parsing failed trying to parse using another logic.\")\n        potential_class_labels = [label.lower() for label in class_label.split() if label in self.classification_labels]\n        class_label = potential_class_labels[0] if potential_class_labels else \"ERROR\"\n\n    return class_label\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.Qwen2VLForImageClassification.generate","title":"<code>generate(image_paths, **kwargs)</code>","text":"<p>Generates class label for a list of images. </p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths to generate class labels for.</p> required <code>**kwargs</code> <p>Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Dict]</code> <p>List[Dict]: List of class labels, validation reasoning and confidence scores for each image.</p> Source code in <code>swiftannotate/image/classification/qwen.py</code> <pre><code>def generate(self, image_paths: List[str], **kwargs) -&gt; List[Dict]:\n    \"\"\"\n    Generates class label for a list of images. \n\n    Args:\n        image_paths (List[str]): List of image paths to generate class labels for.\n        **kwargs: Additional arguments to pass to the method for custom pipeline interactions. To control generation parameters for the model.\n\n    Returns:\n        List[Dict]: List of class labels, validation reasoning and confidence scores for each image.\n    \"\"\"\n    results = super().generate(\n        image_paths=image_paths, \n        **kwargs\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.classification.Qwen2VLForImageClassification.validate","title":"<code>validate(image, class_label, **kwargs)</code>","text":"<p>Validates the class label generated for the image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>Base64 encoded image.</p> required <code>class_label</code> <code>str</code> <p>Class Label generated for the image.</p> required <p>Returns:</p> Type Description <code>Tuple[str, float]</code> <p>Tuple[str, float]: Validation reasoning and confidence score for the class label.</p> Source code in <code>swiftannotate/image/classification/qwen.py</code> <pre><code>def validate(self, image: str, class_label: str, **kwargs) -&gt; Tuple[str, float]:\n    \"\"\"\n    Validates the class label generated for the image.\n\n    Args:\n        image (str): Base64 encoded image.\n        class_label (str): Class Label generated for the image.\n\n    Returns:\n        Tuple[str, float]: Validation reasoning and confidence score for the class label.\n    \"\"\"\n    messages = [\n        {\"role\": \"system\", \"content\": self.validation_prompt},\n        {\n            \"role\": \"user\", \n            \"content\": [\n                {\n                    \"type\": \"image\", \n                    \"image\": f\"data:image;base64,{image}\",\n                    \"resized_height\": self.resize_height,\n                    \"resized_width\": self.resize_width,\n                },\n                {\"type\": \"text\", \"text\": class_label},\n                {\n                    \"type\": \"text\", \n                    \"text\": \"\"\"\n                    Validate the class label generated for the given image. \n                    Return output as a JSON object with keys as 'validation_reasoning' and 'confidence'.\n                    \"\"\"\n                },\n            ],\n        },\n    ]\n\n    text = self.processor.apply_chat_template(\n        messages, tokenize=False, add_generation_prompt=True\n    )\n\n    image_inputs, video_inputs = process_vision_info(messages)\n    inputs = self.processor(\n        text=[text],\n        images=image_inputs,\n        videos=video_inputs,\n        padding=True,\n        return_tensors=\"pt\",\n    )\n    inputs = inputs.to(self.model.device)\n\n    # Inference: Generation of the output\n    if \"max_new_tokens\" not in kwargs:\n        kwargs[\"max_new_tokens\"] = 512\n\n    generated_ids = self.model.generate(**inputs, **kwargs)\n    generated_ids_trimmed = [\n        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n    ]\n    validation_output = self.processor.batch_decode(\n        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n    )[0]\n\n    # TODO: Need a better way to parse the output\n    try:\n        validation_output = validation_output.replace('```', '').replace('json', '')\n        validation_output = json.loads(validation_output)\n        validation_reasoning = validation_output[\"validation_reasoning\"]\n        confidence = validation_output[\"confidence\"]\n    except Exception as e:\n        logger.error(f\"Image class label validation parsing failed trying to parse using another logic.\")\n\n        number_str  = ''.join((ch if ch in '0123456789.-e' else ' ') for ch in validation_output)\n        number_str = [i for i in number_str.split() if i.isalnum()]\n        potential_confidence_scores = [float(i) for i in number_str if float(i) &gt;= 0 and float(i) &lt;= 1]\n        confidence = max(potential_confidence_scores) if potential_confidence_scores else 0.0\n        validation_reasoning = validation_output\n\n    return validation_reasoning, confidence\n</code></pre>"},{"location":"image/#object-detection","title":"Object Detection","text":""},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection","title":"<code>OwlV2ForObjectDetection</code>","text":"<p>               Bases: <code>BaseObjectDetection</code></p> Source code in <code>swiftannotate/image/object_detection/owlv2.py</code> <pre><code>class OwlV2ForObjectDetection(BaseObjectDetection):\n    def __init__(\n        self,\n        model: Owlv2ForObjectDetection,\n        processor: Owlv2Processor,\n        class_labels: List[str],\n        confidence_threshold: float = 0.5,\n        validation: bool = False,\n        validation_prompt: str | None = None,\n        validation_threshold: float | None = None,\n        output_file: str | None = None\n    ):\n        \"\"\"\n        Initialize the OwlV2ObjectDetection class.\n\n        Args:\n            model (Owlv2ForObjectDetection):\n                OwlV2 Object Detection model from Transformers.\n            processor (Owlv2Processor): \n                OwlV2 Processor for Object Detection.\n            class_labels (List[str]): \n                List of class labels.\n            confidence_threshold (float, optional): \n                Minimum confidence threshold for object detection. \n                Defaults to 0.5.\n            validation (bool, optional): \n                Whether to validate annotations from OwlV2. \n                Defaults to False.\n            validation_prompt (str | None, optional): \n                Prompt to validate annotations. \n                Defaults to None.\n            validation_threshold (float | None, optional): \n                Threshold score for annotation validation. \n                Defaults to None.\n            output_file (str | None, optional): \n                Path to save results.\n                If None, results are not saved. Defaults to None.\n\n        Raises:\n            ValueError: If model is not an instance of Owlv2ForObjectDetection.\n            ValueError: If processor is not an instance of Owlv2Processor.\n        \"\"\"\n        if not isinstance(model, Owlv2ForObjectDetection):\n            raise ValueError(\"Model must be an instance of Owlv2ForObjectDetection\")\n        if not isinstance(processor, Owlv2Processor):\n            raise ValueError(\"Processor must be an instance of Owlv2Processor\")\n\n        self.processor = processor\n        self.model = model\n        self.model.eval()\n\n        super().__init__(\n            class_labels,\n            confidence_threshold,\n            validation,\n            validation_prompt,\n            validation_threshold,\n            output_file\n        )\n\n    def annotate(self, image: Image.Image) -&gt; List[dict]:\n        \"\"\"\n        Annotate an image with object detection labels\n\n        Args:\n            image (Image.Image): Image to be annotated.\n\n        Returns:\n            List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n        \"\"\"\n        inputs = self.processor(text=self.class_labels, images=image, return_tensors=\"pt\").to(self.model.device)\n\n        with torch.no_grad():\n            outputs = self.model(**inputs)\n\n        target_sizes = torch.Tensor([image.size[::-1]])\n        results = self.processor.post_process_object_detection(\n            outputs=outputs, \n            target_sizes=target_sizes, \n            threshold=self.confidence_threshold\n        )\n        return [{k: v.cpu().tolist() for k, v in prediction.items()} for prediction in results]\n\n    def validate(self, image: Image.Image, annotations: List[dict]) -&gt; Tuple:\n        \"\"\"\n        Validate the annotations for an image with object detection labels.\n\n        Currently, there is no validation method available for Object Detection.\n\n        # TODO: Idea is to do some sort of object extraction using annotations and ask VLM to validate the extracted objects.\n        # TODO: Need to figure out a way to use the VLM output for improving annotations.\n\n        Args:\n            image (Image.Image): Image to be validated.\n            annotations (List[dict]): List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        raise NotImplementedError(\"No validation method available for Object Detection yet\")\n\n    def generate(self, image_paths: List[str]) -&gt; List[dict]:\n        \"\"\"\n        Generate annotations for a list of image paths.\n\n        Args:\n            image_paths (List[str]): List of image paths.\n\n        Returns:\n            List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n        \"\"\"\n        results = super().generate(\n            image_paths\n        )\n\n        return results\n</code></pre>"},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.__init__","title":"<code>__init__(model, processor, class_labels, confidence_threshold=0.5, validation=False, validation_prompt=None, validation_threshold=None, output_file=None)</code>","text":"<p>Initialize the OwlV2ObjectDetection class.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Owlv2ForObjectDetection</code> <p>OwlV2 Object Detection model from Transformers.</p> required <code>processor</code> <code>Owlv2Processor</code> <p>OwlV2 Processor for Object Detection.</p> required <code>class_labels</code> <code>List[str]</code> <p>List of class labels.</p> required <code>confidence_threshold</code> <code>float</code> <p>Minimum confidence threshold for object detection.  Defaults to 0.5.</p> <code>0.5</code> <code>validation</code> <code>bool</code> <p>Whether to validate annotations from OwlV2.  Defaults to False.</p> <code>False</code> <code>validation_prompt</code> <code>str | None</code> <p>Prompt to validate annotations.  Defaults to None.</p> <code>None</code> <code>validation_threshold</code> <code>float | None</code> <p>Threshold score for annotation validation.  Defaults to None.</p> <code>None</code> <code>output_file</code> <code>str | None</code> <p>Path to save results. If None, results are not saved. Defaults to None.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If model is not an instance of Owlv2ForObjectDetection.</p> <code>ValueError</code> <p>If processor is not an instance of Owlv2Processor.</p> Source code in <code>swiftannotate/image/object_detection/owlv2.py</code> <pre><code>def __init__(\n    self,\n    model: Owlv2ForObjectDetection,\n    processor: Owlv2Processor,\n    class_labels: List[str],\n    confidence_threshold: float = 0.5,\n    validation: bool = False,\n    validation_prompt: str | None = None,\n    validation_threshold: float | None = None,\n    output_file: str | None = None\n):\n    \"\"\"\n    Initialize the OwlV2ObjectDetection class.\n\n    Args:\n        model (Owlv2ForObjectDetection):\n            OwlV2 Object Detection model from Transformers.\n        processor (Owlv2Processor): \n            OwlV2 Processor for Object Detection.\n        class_labels (List[str]): \n            List of class labels.\n        confidence_threshold (float, optional): \n            Minimum confidence threshold for object detection. \n            Defaults to 0.5.\n        validation (bool, optional): \n            Whether to validate annotations from OwlV2. \n            Defaults to False.\n        validation_prompt (str | None, optional): \n            Prompt to validate annotations. \n            Defaults to None.\n        validation_threshold (float | None, optional): \n            Threshold score for annotation validation. \n            Defaults to None.\n        output_file (str | None, optional): \n            Path to save results.\n            If None, results are not saved. Defaults to None.\n\n    Raises:\n        ValueError: If model is not an instance of Owlv2ForObjectDetection.\n        ValueError: If processor is not an instance of Owlv2Processor.\n    \"\"\"\n    if not isinstance(model, Owlv2ForObjectDetection):\n        raise ValueError(\"Model must be an instance of Owlv2ForObjectDetection\")\n    if not isinstance(processor, Owlv2Processor):\n        raise ValueError(\"Processor must be an instance of Owlv2Processor\")\n\n    self.processor = processor\n    self.model = model\n    self.model.eval()\n\n    super().__init__(\n        class_labels,\n        confidence_threshold,\n        validation,\n        validation_prompt,\n        validation_threshold,\n        output_file\n    )\n</code></pre>"},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.annotate","title":"<code>annotate(image)</code>","text":"<p>Annotate an image with object detection labels</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image to be annotated.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.</p> Source code in <code>swiftannotate/image/object_detection/owlv2.py</code> <pre><code>def annotate(self, image: Image.Image) -&gt; List[dict]:\n    \"\"\"\n    Annotate an image with object detection labels\n\n    Args:\n        image (Image.Image): Image to be annotated.\n\n    Returns:\n        List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n    \"\"\"\n    inputs = self.processor(text=self.class_labels, images=image, return_tensors=\"pt\").to(self.model.device)\n\n    with torch.no_grad():\n        outputs = self.model(**inputs)\n\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = self.processor.post_process_object_detection(\n        outputs=outputs, \n        target_sizes=target_sizes, \n        threshold=self.confidence_threshold\n    )\n    return [{k: v.cpu().tolist() for k, v in prediction.items()} for prediction in results]\n</code></pre>"},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.generate","title":"<code>generate(image_paths)</code>","text":"<p>Generate annotations for a list of image paths.</p> <p>Parameters:</p> Name Type Description Default <code>image_paths</code> <code>List[str]</code> <p>List of image paths.</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.</p> Source code in <code>swiftannotate/image/object_detection/owlv2.py</code> <pre><code>def generate(self, image_paths: List[str]) -&gt; List[dict]:\n    \"\"\"\n    Generate annotations for a list of image paths.\n\n    Args:\n        image_paths (List[str]): List of image paths.\n\n    Returns:\n        List[dict]: List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n    \"\"\"\n    results = super().generate(\n        image_paths\n    )\n\n    return results\n</code></pre>"},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.validate","title":"<code>validate(image, annotations)</code>","text":"<p>Validate the annotations for an image with object detection labels.</p> <p>Currently, there is no validation method available for Object Detection.</p>"},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.validate--todo-idea-is-to-do-some-sort-of-object-extraction-using-annotations-and-ask-vlm-to-validate-the-extracted-objects","title":"TODO: Idea is to do some sort of object extraction using annotations and ask VLM to validate the extracted objects.","text":""},{"location":"image/#swiftannotate.image.object_detection.OwlV2ForObjectDetection.validate--todo-need-to-figure-out-a-way-to-use-the-vlm-output-for-improving-annotations","title":"TODO: Need to figure out a way to use the VLM output for improving annotations.","text":"<p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Image to be validated.</p> required <code>annotations</code> <code>List[dict]</code> <p>List of dictionaries containing the confidence scores, bounding box coordinates and class labels.</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> Source code in <code>swiftannotate/image/object_detection/owlv2.py</code> <pre><code>def validate(self, image: Image.Image, annotations: List[dict]) -&gt; Tuple:\n    \"\"\"\n    Validate the annotations for an image with object detection labels.\n\n    Currently, there is no validation method available for Object Detection.\n\n    # TODO: Idea is to do some sort of object extraction using annotations and ask VLM to validate the extracted objects.\n    # TODO: Need to figure out a way to use the VLM output for improving annotations.\n\n    Args:\n        image (Image.Image): Image to be validated.\n        annotations (List[dict]): List of dictionaries containing the confidence scores, bounding box coordinates and class labels.\n\n    Raises:\n        NotImplementedError: _description_\n    \"\"\"\n    raise NotImplementedError(\"No validation method available for Object Detection yet\")\n</code></pre>"}]}